{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import pyvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = r\"C:\\Users\\Joycelyn\\Documents\\Synapxe\\07 GenAI for healthhub\\Github backup\\cluster experiments\\weighted emb vs sim (658)\"\n",
    "emb_path = os.path.join(root_path,\"nomic_weighted_emb (0.7 body, 0.3 title)\")\n",
    "sim_path = os.path.join(root_path,\"nomic_weighted_sim (0.7 body, 0.3 title)\")\n",
    "\n",
    "with open(os.path.join(emb_path,\"neo4j_predicted_clusters.pkl\"), \"rb\") as f:\n",
    "    emb_results = pickle.load(f)\n",
    "\n",
    "with open(os.path.join(sim_path,\"neo4j_predicted_clusters.pkl\"), \"rb\") as f:\n",
    "    sim_results = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## identify common and unique clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_common_clusters(output1, output2):\n",
    "    clusters1 = {tuple(cluster['titles']): cluster for cluster in output1}\n",
    "    clusters2 = {tuple(cluster['titles']): cluster for cluster in output2}\n",
    "    \n",
    "    common_titles = set(clusters1.keys()) & set(clusters2.keys())\n",
    "    \n",
    "    common_clusters = [clusters1[titles] for titles in common_titles]\n",
    "    df_common_clusters = pd.DataFrame(common_clusters)\n",
    "\n",
    "    df_common_clusters['num_articles'] = df_common_clusters['titles'].apply(lambda x: len(x))\n",
    "\n",
    "    return df_common_clusters\n",
    "\n",
    "def find_different_clusters(output1, output2):\n",
    "    clusters1 = {tuple(cluster['titles']): cluster for cluster in output1}\n",
    "    clusters2 = {tuple(cluster['titles']): cluster for cluster in output2}\n",
    "    \n",
    "    unique_titles1 = set(clusters1.keys()) - set(clusters2.keys())\n",
    "    unique_titles2 = set(clusters2.keys()) - set(clusters1.keys())\n",
    "    \n",
    "    unique_clusters1 = [clusters1[titles] for titles in unique_titles1]\n",
    "    unique_clusters2 = [clusters2[titles] for titles in unique_titles2]\n",
    "\n",
    "    df_unique_clusters1 = pd.DataFrame(unique_clusters1)\n",
    "    df_unique_clusters2 = pd.DataFrame(unique_clusters2)\n",
    "\n",
    "    df_unique_clusters1['num_articles'] = df_unique_clusters1['titles'].apply(lambda x: len(x))\n",
    "    df_unique_clusters2['num_articles'] = df_unique_clusters2['titles'].apply(lambda x: len(x))\n",
    "\n",
    "    return df_unique_clusters1, df_unique_clusters2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of exact same clusters: 24     \n",
      "Number of unique clusters from EMB: 48     \n",
      "Number of unique clusters from SIM: 30\n"
     ]
    }
   ],
   "source": [
    "df_common_clusters = find_common_clusters(emb_results, sim_results)\n",
    "df_unique_emb, df_unique_sim = find_different_clusters(emb_results, sim_results)\n",
    "\n",
    "print(f\"Number of exact same clusters: {len(df_common_clusters)} \\\n",
    "    \\nNumber of unique clusters from EMB: {len(df_unique_emb)} \\\n",
    "    \\nNumber of unique clusters from SIM: {len(df_unique_sim)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "# match unique clusters\n",
    "df_unique_emb_1 = df_unique_emb.copy()\n",
    "df_unique_sim_1 = df_unique_sim.copy()\n",
    "\n",
    "# Function to calculate overlap\n",
    "def calculate_overlap(row1, row2):\n",
    "    return len(set(row1) & set(row2))\n",
    "\n",
    "# List to store the matches\n",
    "matches = []\n",
    "\n",
    "for i, row1 in df_unique_emb_1.iterrows():\n",
    "    for j, row2 in df_unique_sim_1.iterrows():\n",
    "        overlap = calculate_overlap(row1['titles'], row2['titles'])\n",
    "        matches.append((i, j, overlap))\n",
    "\n",
    "matches_df = pd.DataFrame(matches, columns=['embeddings_index', 'sim_index', 'overlap'])\n",
    "\n",
    "# Identify the pairs with the highest overlap\n",
    "max_matches_df = matches_df.loc[matches_df.groupby('embeddings_index')['overlap'].idxmax()]\n",
    "# Sort by sim_index and overlap and keep the sim_index with highest overlap\n",
    "max_matches_df = max_matches_df[max_matches_df['overlap'] != 0].sort_values(['sim_index', 'overlap'], ascending=[True, False])\n",
    "max_matches_df = max_matches_df.drop_duplicates(subset=['sim_index'], keep='first')  \n",
    "max_matches_df\n",
    "\n",
    "# Prepare data for full outer join\n",
    "df_unique_emb['key'] = df_unique_emb.index\n",
    "df_unique_sim['key'] = df_unique_sim.index\n",
    "\n",
    "# Merge the DataFrames using the identified pairs\n",
    "merged_df_title = pd.merge(\n",
    "    df_unique_emb,\n",
    "    max_matches_df[['embeddings_index', 'sim_index','overlap']],\n",
    "    left_index=True,\n",
    "    right_on='embeddings_index',\n",
    "    how='outer'\n",
    ").merge(\n",
    "    df_unique_sim,\n",
    "    left_on='sim_index',\n",
    "    right_index=True,\n",
    "    how='outer',\n",
    "    suffixes=('_embeddings', '_sim')\n",
    ").drop(columns=['embeddings_index', 'sim_index', 'key_embeddings', 'key_sim'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export dfs\n",
    "save_path = os.path.join(root_path, \"weighted_methods_eval.xlsx\")\n",
    "with pd.ExcelWriter(save_path) as writer:\n",
    "    df_common_clusters.to_excel(writer, sheet_name='Common Clusters', index=False)\n",
    "    df_unique_emb.to_excel(writer, sheet_name='Weighted emb unique articles', index=False)\n",
    "    df_unique_sim.to_excel(writer, sheet_name='Weighted sim unique articles', index=False)\n",
    "    merged_df_title.to_excel(writer, sheet_name='match_unique_clusters_keywords', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## visualise unique cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_result(clustered_df,method:str):\n",
    "    visual_graph = pyvis.network.Network(select_menu=True, filter_menu=True)\n",
    "\n",
    "    # Add nodes-nodes pair\n",
    "    for _, row in clustered_df.iterrows():\n",
    "        # Add nodes\n",
    "        visual_graph.add_node(\n",
    "            row[\"node_1_title\"],\n",
    "            label=row[\"node_1_title\"],\n",
    "            title=f\"\\nPredicted: {row['node_1_pred_cluster']}\\nTitle: {row['node_1_title']}\\nKeywords: {row['node_1_cluster_kws']}\",\n",
    "            group=row[\"node_1_cluster_kws\"],\n",
    "            cluster_num=row[\"node_1_pred_cluster\"]\n",
    "        )\n",
    "        visual_graph.add_node(\n",
    "            row[\"node_2_title\"],\n",
    "            label=row[\"node_2_title\"],\n",
    "            title=f\"\\nPredicted: {row['node_2_pred_cluster']}\\nTitle: {row['node_2_title']}\\nKeywords: {row['node_2_cluster_kws']}\",\n",
    "            group=row[\"node_2_cluster_kws\"],\n",
    "            cluster_num=row[\"node_2_pred_cluster\"]\n",
    "        )\n",
    "\n",
    "        # Add edge\n",
    "        visual_graph.add_edge(\n",
    "            row[\"node_1_title\"],\n",
    "            row[\"node_2_title\"],\n",
    "            title=f\"Edge Weight: {row['edge_weight']}\",\n",
    "        )\n",
    "\n",
    "    visual_graph.show(f\"neo4j_{method}.html\", notebook=False)\n",
    "\n",
    "def get_unique_inter_grp(df):\n",
    "    df = df[df[\"node_1_pred_cluster\"] != df[\"node_2_pred_cluster\"]]\n",
    "    unique_pairs = df[['node_1_pred_cluster', 'node_2_pred_cluster']].drop_duplicates()\n",
    "    num_rows_with_unique_pairs = unique_pairs.shape[0]\n",
    "    return num_rows_with_unique_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustered_nodes_emb = pd.read_csv(os.path.join(emb_path,\"neo_4j_clustered_data.csv\"))\n",
    "clustered_nodes_sim = pd.read_csv(os.path.join(sim_path,\"neo_4j_clustered_data.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of intergroup connections\n",
      "EMB method: 15\n",
      "SIM method: 35\n",
      "\n",
      "Number of unique interconnected groups\n",
      "EMB method: 7\n",
      "SIM method: 15\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of intergroup connections\")\n",
    "print(f\"EMB method: {len(clustered_nodes_emb[clustered_nodes_emb[\"node_1_pred_cluster\"] != clustered_nodes_emb[\"node_2_pred_cluster\"]])}\")\n",
    "print(f\"SIM method: {len(clustered_nodes_sim[clustered_nodes_sim[\"node_1_pred_cluster\"] != clustered_nodes_sim[\"node_2_pred_cluster\"]])}\")\n",
    "\n",
    "print(\"\\nNumber of unique interconnected groups\")\n",
    "print(f\"EMB method: {get_unique_inter_grp(clustered_nodes_emb)}\")\n",
    "print(f\"SIM method: {get_unique_inter_grp(clustered_nodes_sim)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neo4j_emb.html\n"
     ]
    }
   ],
   "source": [
    "# visualisation for unique emb clusters\n",
    "unique_emb_list = list(df_unique_emb[\"cluster\"])\n",
    "clustered_nodes_emb_filtered = clustered_nodes_emb[\n",
    "    clustered_nodes_emb['node_1_pred_cluster'].isin(unique_emb_list) &\n",
    "    clustered_nodes_emb['node_2_pred_cluster'].isin(unique_emb_list)\n",
    "]\n",
    "visualize_result(clustered_nodes_emb_filtered, 'emb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neo4j_sim.html\n"
     ]
    }
   ],
   "source": [
    "# visualisation for unique sim clusters\n",
    "unique_sim_list = list(df_unique_sim[\"cluster\"])\n",
    "clustered_nodes_sim_filtered = clustered_nodes_sim[\n",
    "    clustered_nodes_sim['node_1_pred_cluster'].isin(unique_sim_list) &\n",
    "    clustered_nodes_sim['node_2_pred_cluster'].isin(unique_sim_list)\n",
    "]\n",
    "visualize_result(clustered_nodes_sim_filtered, 'sim')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing unique clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emb cluster size: 41     \n",
      "Sim cluster size: 36     \n",
      "Number of overlap articles: 31\n",
      "\n",
      "Unique articles in EMB cluster:     \n",
      "[\"Life is Better When You're Sober\", 'Alcohol and Health—Set Your Drinking Limits', 'Why is Binge Drinking Bad for You?', 'Know Your Alcohol Limit: Don’t Be a Party Pooper!', 'Drinking (or not) to a Healthy Chinese New Year', 'Alcohol — More than Meets the Eye', 'Drinking Myths Busted!', 'Staying Sober and Within the Alcohol Limit', 'Responsible Drinking: Know Your Alcohol Limit', 'Ditch Both that Cigarette and Drink!']     \n",
      "\n",
      "Unique articles in SIM cluster:     \n",
      "['Environmental Tobacco Smoke', 'Are e-cigarettes harmful?', 'Effects of Secondhand Smoke on Your Child’s Health', 'Smoke-free Environment for a Healthier Family', '\"Vaping is not smoking\", and Other Tobacco Myths']\n"
     ]
    }
   ],
   "source": [
    "emb_cluster_num = 532\n",
    "sim_cluster_num = 592\n",
    "\n",
    "emb_titles_list = df_unique_emb[df_unique_emb[\"cluster\"]==emb_cluster_num][\"titles\"].iloc[0]\n",
    "sim_titles_list = df_unique_sim[df_unique_sim[\"cluster\"]==sim_cluster_num][\"titles\"].iloc[0]\n",
    "\n",
    "emb_set = set(emb_titles_list)\n",
    "sim_set = set(sim_titles_list)\n",
    "common_articles = emb_set.intersection(sim_set)\n",
    "\n",
    "print(f\"Emb cluster size: {len(emb_titles_list)} \\\n",
    "    \\nSim cluster size: {len(sim_titles_list)} \\\n",
    "    \\nNumber of overlap articles: {len(common_articles)}\"\n",
    ")\n",
    "\n",
    "print(f\"\\nUnique articles in EMB cluster: \\\n",
    "    \\n{list(emb_set - sim_set)} \\\n",
    "    \\n\\nUnique articles in SIM cluster: \\\n",
    "    \\n{list(sim_set - emb_set)}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai-hh",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
