{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "%load_ext kedro.ipython"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import re\n",
    "\n",
    "import pandas as pd"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ruff: noqa: F821\n",
    "catalog.list()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ruff: noqa: F821\n",
    "merged_data = catalog.load(\"merged_data\")\n",
    "merged_data = merged_data.drop([\"to_remove\", \"remove_type\"], axis=1)\n",
    "merged_data"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ruff: noqa: F821\n",
    "whitelist = catalog.load(\"params:whitelist\")\n",
    "whitelist"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "merged_data[\"to_remove\"] = False\n",
    "merged_data[\"remove_type\"] = None"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "merged_data[\"content_category\"].value_counts().sort_values(ascending=False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NaN\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "merged_data.groupby(by=\"content_category\")[\"content_body\"].apply(\n",
    "    lambda x: x.isna().sum()\n",
    ").sort_values(ascending=False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Get indexes for all NaN content body\n",
    "na_indexes = merged_data[merged_data[\"content_body\"].isna()].index\n",
    "\n",
    "merged_data.loc[na_indexes, \"to_remove\"] = True\n",
    "merged_data.loc[na_indexes, \"remove_type\"] = \"NaN\""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Excel Error\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "merged_data.groupby(by=\"content_category\")[\"content_body\"].apply(\n",
    "    lambda x: (x.str.contains(\"Value exceeded maximum cell size\")).sum()\n",
    ").sort_values(ascending=False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "content_category = \"live-healthy-articles\"\n",
    "\n",
    "# Define the regex pattern\n",
    "pattern = re.compile(r\"(Value exceeded maximum cell)\")\n",
    "\n",
    "\n",
    "# Function to apply regex and return True or False\n",
    "def apply_regex_v1(x):\n",
    "    if pd.isna(x):\n",
    "        return False\n",
    "    return bool(pattern.search(str(x)))\n",
    "\n",
    "\n",
    "filtered = merged_data[merged_data[\"content_body\"].apply(lambda x: apply_regex_v1(x))]\n",
    "filtered.query(f\"content_category == '{content_category}'\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "filtered.groupby(by=\"content_category\").size().sort_values(ascending=False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Either NaN or Value exceeded maximum cell\n",
    "merged_data.groupby(by=\"content_category\")[\"content_body\"].apply(\n",
    "    lambda x: (x.isna() | x.str.contains(\"Value exceeded maximum cell\")).sum()\n",
    ").sort_values(ascending=False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# All Excel Error indexes\n",
    "excel_error = \"Value exceeded maximum cell size\"\n",
    "excel_error_indexes = merged_data.query(\n",
    "    f\"content_body.str.contains('{excel_error}', na=False)\"\n",
    ").index\n",
    "\n",
    "merged_data.loc[excel_error_indexes, \"to_remove\"] = True\n",
    "merged_data.loc[excel_error_indexes, \"remove_type\"] = \"Excel Error\""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "excel_error_indexes"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## No Tags\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Define the regex pattern\n",
    "pattern = re.compile(r\"(<[div|p|h2].*?>)\")\n",
    "\n",
    "\n",
    "# Function to apply regex and return True or False\n",
    "def apply_regex_v2(x):\n",
    "    if pd.isna(x) or \"Value exceeded maximum cell size\" in str(x):\n",
    "        return True\n",
    "    return bool(pattern.search(str(x)))\n",
    "\n",
    "\n",
    "# Group by content_category and apply the regex\n",
    "grouped = merged_data.groupby(by=\"content_category\")[\"content_body\"].apply(\n",
    "    lambda x: ~x.apply(apply_regex_v2)\n",
    ")\n",
    "\n",
    "grouped.groupby(by=\"content_category\").value_counts().loc[:, True].sort_values(\n",
    "    ascending=False\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# All content without HTML tags indexes\n",
    "no_tags_indexes = merged_data[\n",
    "    ~merged_data.query(\"content_category.notna()\")[\"content_body\"].apply(\n",
    "        lambda x: apply_regex_v2(x)\n",
    "    )\n",
    "].index\n",
    "\n",
    "merged_data.loc[no_tags_indexes, \"to_remove\"] = True\n",
    "merged_data.loc[no_tags_indexes, \"remove_type\"] = \"No HTML Tags\""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## No Extracted Content\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# All content ids without extracted content\n",
    "no_extracted_content_ids = set(\n",
    "    merged_data[merged_data[\"extracted_content_body\"] == \"\"].id.to_list()\n",
    ").difference(set(whitelist))\n",
    "\n",
    "# All content without extracted content indexes\n",
    "no_extracted_content_indexes = merged_data.query(\n",
    "    \"id in @no_extracted_content_ids\"\n",
    ").index\n",
    "\n",
    "merged_data.loc[no_extracted_content_indexes, \"to_remove\"] = True\n",
    "merged_data.loc[no_extracted_content_indexes, \"remove_type\"] = \"No Extracted Content\""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "merged_data.groupby(by=\"content_category\")[\"remove_type\"].apply(\n",
    "    lambda x: (x == \"No Extracted Content\").sum()\n",
    ").sort_values(ascending=False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Duplicated Content\n",
    "\n",
    "**Note:** Done across content categories <u>not</u> together, merged.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "duplicated = pd.DataFrame()\n",
    "\n",
    "for content_category in merged_data[\"content_category\"].unique():\n",
    "    tmp = merged_data.query(\"content_category == @content_category\")\n",
    "    tmp = tmp[\n",
    "        (tmp[\"extracted_content_body\"].duplicated())\n",
    "        & (tmp[\"extracted_content_body\"].notna())\n",
    "        & (tmp[\"extracted_content_body\"] != \"\")\n",
    "        & (~tmp[\"to_remove\"])\n",
    "    ]\n",
    "\n",
    "    duplicated = pd.concat([duplicated, tmp], axis=0)\n",
    "\n",
    "duplicated"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "merged_data[\"duplicates_with\"] = None\n",
    "\n",
    "for i in range(len(duplicated)):\n",
    "    tmp = merged_data[\n",
    "        (\n",
    "            merged_data[\"extracted_content_body\"]\n",
    "            == duplicated.iloc[i][\"extracted_content_body\"]\n",
    "        )\n",
    "        & (merged_data[\"content_category\"] == duplicated.iloc[i][\"content_category\"])\n",
    "    ]\n",
    "\n",
    "    indexes = tmp.index.tolist()\n",
    "\n",
    "    for index in indexes:\n",
    "        curr_id = merged_data.iloc[index][\"id\"]\n",
    "\n",
    "        # Ignore whitelisted articles\n",
    "        if curr_id in whitelist:\n",
    "            continue\n",
    "\n",
    "        all_ids = merged_data.loc[indexes, \"id\"].to_list()\n",
    "\n",
    "        # Remove current ID\n",
    "        all_ids.remove(curr_id)\n",
    "\n",
    "        if not merged_data.iloc[index][\"to_remove\"]:\n",
    "            # Update `to_remove`\n",
    "            merged_data.at[index, \"to_remove\"] = True\n",
    "\n",
    "            # Update `remove_type` to \"Duplicated Content\"\n",
    "            merged_data.at[index, \"remove_type\"] = \"Duplicated Content\"\n",
    "\n",
    "            # Update column `duplicates_with` to article IDs\n",
    "            merged_data.at[index, \"duplicates_with\"] = all_ids"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "tmp = merged_data.query(\"remove_type == 'Duplicated Content'\")\n",
    "tmp.groupby(by=\"content_category\").size().sort_values(ascending=False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Duplicated URLs\n",
    "\n",
    "**Note:** Done across content categories <u>not</u> together, merged.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "duplicated = pd.DataFrame()\n",
    "\n",
    "for content_category in merged_data[\"content_category\"].unique():\n",
    "    tmp = merged_data.query(\"content_category == @content_category\")\n",
    "    tmp = tmp[\n",
    "        (tmp[\"full_url\"].duplicated()) & (tmp[\"full_url\"].notna()) & (~tmp[\"to_remove\"])\n",
    "    ]\n",
    "\n",
    "    duplicated = pd.concat([duplicated, tmp], axis=0)\n",
    "\n",
    "duplicated"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "for i in range(len(duplicated)):\n",
    "    tmp = merged_data[\n",
    "        (merged_data[\"full_url\"] == duplicated.iloc[i][\"full_url\"])\n",
    "        & (merged_data[\"content_category\"] == duplicated.iloc[i][\"content_category\"])\n",
    "    ]\n",
    "\n",
    "    indexes = tmp.index.tolist()\n",
    "\n",
    "    for index in indexes:\n",
    "        curr_id = merged_data.iloc[index][\"id\"]\n",
    "\n",
    "        # Ignore whitelisted articles\n",
    "        if curr_id in whitelist:\n",
    "            continue\n",
    "\n",
    "        all_ids = merged_data.loc[indexes, \"id\"].to_list()\n",
    "\n",
    "        # Remove current ID\n",
    "        all_ids.remove(curr_id)\n",
    "\n",
    "        if not merged_data.iloc[index][\"to_remove\"]:\n",
    "            # Update `to_remove`\n",
    "            merged_data.at[index, \"to_remove\"] = True\n",
    "\n",
    "            # Update `remove_type` to \"Duplicated URL\"\n",
    "            merged_data.at[index, \"remove_type\"] = \"Duplicated URL\"\n",
    "\n",
    "            # Update column `duplicates_with` to article IDs\n",
    "            merged_data.at[index, \"duplicates_with\"] = all_ids"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "tmp = merged_data.query(\"remove_type == 'Duplicated URL'\")\n",
    "tmp.groupby(by=\"content_category\").size().sort_values(ascending=False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Below Word Count\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ruff: noqa: F821\n",
    "word_count_cutoff = catalog.load(\"params:word_count_cutoff\")\n",
    "word_count_cutoff"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Assuming merged_data is your DataFrame and `word_count_cutoff` is defined\n",
    "indices = merged_data.query(\n",
    "    \"extracted_content_body.notna() \"\n",
    "    \"and remove_type != 'Duplicated Content' \"\n",
    "    \"and remove_type != 'Duplicated URL'\"\n",
    ")[\"extracted_content_body\"].apply(\n",
    "    lambda x: len(x.split()) > 0 and len(x.split()) <= word_count_cutoff\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Get the indices of the True values\n",
    "below_word_count_indexes = indices[indices].index\n",
    "\n",
    "# All content ids below word count cutoff\n",
    "below_word_count_ids = set(\n",
    "    merged_data.iloc[below_word_count_indexes].id.to_list()\n",
    ").difference(set(whitelist))\n",
    "\n",
    "# All content below word count cutoff indexes\n",
    "below_word_count_indexes = merged_data.query(\"id in @below_word_count_ids\").index\n",
    "\n",
    "merged_data.loc[below_word_count_indexes, \"to_remove\"] = True\n",
    "merged_data.loc[below_word_count_indexes, \"remove_type\"] = \"Below Word Count\""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "tmp = merged_data.query(\"remove_type == 'Below Word Count'\")\n",
    "tmp.groupby(by=\"content_category\").size().sort_values(ascending=False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples (Excel Error)\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "content_category = \"diseases-and-conditions\"\n",
    "\n",
    "tmp = merged_data.query(\n",
    "    f\"content_category == '{content_category}' and remove_type == 'Excel Error'\"\n",
    ")\n",
    "tmp"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "tmp[\"content_body\"].values.tolist()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "tmp[\"title\"].values.tolist()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "tmp[\"full_url\"].values.tolist()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples (No Tags)\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "content_category = \"cost-and-financing\"\n",
    "\n",
    "tmp = merged_data.query(\n",
    "    f\"content_category == '{content_category}' and remove_type == 'No HTML Tags'\"\n",
    ")\n",
    "tmp"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(tmp[\"content_body\"].values.tolist()[0])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "tmp[\"title\"].values.tolist()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "tmp[\"full_url\"].values.tolist()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples (No Extracted Content)\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "merged_data.query(f\"content_category == '{content_category}'\")[\n",
    "    \"remove_type\"\n",
    "].value_counts()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "content_category = \"support-group-and-others\"\n",
    "\n",
    "tmp = merged_data.query(\n",
    "    f\"content_category == '{content_category}' and remove_type == 'No Extracted Content'\"\n",
    ")\n",
    "tmp"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "tmp[\"content_body\"].values.tolist()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "tmp[\"title\"].values.tolist()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "tmp[\"full_url\"].values.tolist()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples (Duplicated Content)\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "merged_data.query(\"remove_type == 'Duplicated Content'\")[\"title\"].values.tolist()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "merged_data.query(\"remove_type == 'Duplicated Content'\")[\"full_url\"].values.tolist()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples (Duplicated URL)\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "merged_data.query(\"remove_type == 'Duplicated URL'\")[\"title\"].values.tolist()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "merged_data.query(\"remove_type == 'Duplicated URL'\")[\"full_url\"].values.tolist()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples (Below Word Count)\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "merged_data[\"remove_type\"].value_counts()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "tmp = merged_data.query(\n",
    "    \"extracted_content_body.notna() \"\n",
    "    \"and remove_type != 'Duplicated Content' \"\n",
    "    \"and remove_type != 'Duplicated URL'\"\n",
    ")\n",
    "tmp = tmp[\n",
    "    tmp[\"extracted_content_body\"].apply(\n",
    "        lambda x: len(x.split()) > 0 and len(x.split()) <= word_count_cutoff\n",
    "    )\n",
    "]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "content_category = \"diseases-and-conditions\"\n",
    "\n",
    "sampled = tmp.query(f\"content_category == '{content_category}'\").sample(1)\n",
    "\n",
    "print(sampled.index[0])\n",
    "print(sampled[\"full_url\"].values[0])\n",
    "print(sampled[\"title\"].values[0])\n",
    "print(sampled[\"extracted_content_body\"].values[0])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Excel with `remove_type` column for VML\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "merged_data.shape"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "merged_data.groupby(by=\"content_category\")[\"to_remove\"].value_counts().loc[\n",
    "    :, True\n",
    "].sort_values(ascending=False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "(\n",
    "    merged_data.groupby(by=\"content_category\")[\"to_remove\"]\n",
    "    .value_counts(normalize=True)\n",
    "    .loc[:, True]\n",
    "    .sort_values(ascending=False)\n",
    "    * 100.00\n",
    ").round(2)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "merged_data[\"remove_type\"].value_counts()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "merged_data[\"remove_type\"].value_counts().sum()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "merged_data.columns"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "flag_for_removal_by_type = merged_data[\n",
    "    [\n",
    "        \"id\",\n",
    "        \"content_category\",\n",
    "        \"pr_name\",\n",
    "        \"content_name\",\n",
    "        \"title\",\n",
    "        \"full_url\",\n",
    "        \"content_body\",\n",
    "        \"extracted_content_body\",\n",
    "        \"to_remove\",\n",
    "        \"remove_type\",\n",
    "        \"duplicates_with\",\n",
    "    ]\n",
    "]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# # # ruff: noqa: F821\n",
    "# catalog.save(\"flag_for_removal_by_type\", flag_for_removal_by_type)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ruff: noqa: F821\n",
    "flag_for_removal_by_type = catalog.load(\"flag_for_removal_by_type\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "flag_for_removal_by_type.query(\"to_remove == True\")[\"content_category\"].value_counts()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "content-opt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
