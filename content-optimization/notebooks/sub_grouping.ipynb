{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "import hdbscan\n",
    "import pandas as pd\n",
    "\n",
    "from umap import UMAP\n",
    "from hdbscan import HDBSCAN\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from bertopic import BERTopic\n",
    "from bertopic.representation import MaximalMarginalRelevance\n",
    "from bertopic.vectorizers import ClassTfidfTransformer\n",
    "\n",
    "import numpy as np\n",
    "import pyvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = \"../data\"\n",
    "experiment_path = os.path.join(root_path,\"07_model_output\",\"all-MiniLM-L6-v2\",\"experiment-5b-weighted-similarities-3_title_7_body\")\n",
    "weighted_embeddings_path = os.path.join(root_path, \"04_feature\", \"weighted_embeddings.pkl\")\n",
    "neo4j_predicted_cluster_pkl_path = os.path.join(experiment_path, \"neo4j_predicted_clusters.pkl\")\n",
    "\n",
    "with open(weighted_embeddings_path, \"rb\") as f:\n",
    "    weighted_embeddings = pickle.load(f)\n",
    "\n",
    "with open(neo4j_predicted_cluster_pkl_path, \"rb\") as f:\n",
    "    neo4j_predicted_cluster_pkl = pickle.load(f)\n",
    "\n",
    "pred_cluster_df = pd.read_csv(os.path.join(experiment_path, \"predicted_cluster.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of cluster to do 2nd level clustering:  10\n"
     ]
    }
   ],
   "source": [
    "cluster_size_count = pred_cluster_df.cluster.value_counts()\n",
    "to_keep = cluster_size_count[cluster_size_count >10].index\n",
    "cluster_morethan10 = pred_cluster_df[pred_cluster_df.cluster.isin(to_keep)]\n",
    "print('No. of cluster to do 2nd level clustering: ', cluster_morethan10.cluster.nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "cluster_morethan10_embeddings = pd.merge(\n",
    "    cluster_morethan10,\n",
    "    weighted_embeddings[['id','vector_extracted_content_body']],\n",
    "    how='left',\n",
    "    on='id')\n",
    "\n",
    "print(cluster_morethan10.shape[0] == cluster_morethan10_embeddings.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(cluster_df):\n",
    "    embeddings = np.array(cluster_df.vector_extracted_content_body.to_list())\n",
    "    doc_titles = cluster_df.title.to_list()\n",
    "    docs = cluster_df.body_content.to_list()\n",
    "    ids = cluster_df.id.to_list()\n",
    "    umap_model = UMAP(n_neighbors=10, n_components=5, min_dist=0.0, metric='cosine', random_state=42)\n",
    "    umap_embeddings = umap_model.fit_transform(embeddings)\n",
    "\n",
    "    return embeddings, doc_titles, docs, ids, umap_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyperparameter_tuning(embeddings):\n",
    "    best_score = 0\n",
    "\n",
    "    for min_cluster_size in [2,3,4,5,6]:\n",
    "        for min_samples in [1,2,3,4,5,6,7]:\n",
    "            for cluster_selection_method in ['leaf']:\n",
    "                for metric in ['euclidean','manhattan']:\n",
    "                    # for each combination of parameters of hdbscan\n",
    "                    hdb = hdbscan.HDBSCAN(min_cluster_size=min_cluster_size,min_samples=min_samples,\n",
    "                                        cluster_selection_method=cluster_selection_method, metric=metric, \n",
    "                                        gen_min_span_tree=True).fit(embeddings)\n",
    "                    # DBCV score\n",
    "                    score = hdb.relative_validity_\n",
    "                    if score > best_score:\n",
    "                        best_score = score\n",
    "                        best_parameters = {'min_cluster_size': min_cluster_size, \n",
    "                                'min_samples':  min_samples, 'cluster_selection_method': cluster_selection_method,\n",
    "                                'metric': metric}\n",
    "\n",
    "    print(\"Best DBCV score: {:.3f}\".format(best_score))\n",
    "    print(\"Best parameters: {}\".format(best_parameters))\n",
    "    return best_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topic_modelling(hyperparameters):\n",
    "    # Step 3 - Cluster reduced embeddings\n",
    "    hdbscan_model = HDBSCAN(min_cluster_size=hyperparameters['min_cluster_size'], min_samples=hyperparameters['min_samples'], metric=hyperparameters['metric'], cluster_selection_method=hyperparameters['cluster_selection_method'], prediction_data=True, gen_min_span_tree=True)\n",
    "\n",
    "    # Step 4 - Tokenize topics\n",
    "    vectorizer_model = CountVectorizer(stop_words=\"english\")\n",
    "\n",
    "    # Step 5 - Create topic representation\n",
    "    ctfidf_model = ClassTfidfTransformer()\n",
    "\n",
    "    # Step 6 - (Optional) Fine-tune topic representations with \n",
    "    representation_model = MaximalMarginalRelevance(diversity=0.3)\n",
    "\n",
    "    # All steps together\n",
    "    topic_model = BERTopic(\n",
    "    # embedding_model=embedding_model,          # Step 1 - Extract embeddings\n",
    "    # umap_model=umap_model,                    # Step 2 - Reduce dimensionality\n",
    "    hdbscan_model=hdbscan_model,              # Step 3 - Cluster reduced embeddings\n",
    "    vectorizer_model=vectorizer_model,        # Step 4 - Tokenize topics\n",
    "    ctfidf_model=ctfidf_model,                # Step 5 - Extract topic words\n",
    "    representation_model=representation_model, # Step 6 - (Optional) Fine-tune topic represenations\n",
    "    # nr_topics=\"auto\" #default is none, will auto reduce topics using HDBSCAN\n",
    "    )\n",
    "    return topic_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best DBCV score: 0.148\n",
      "Best parameters: {'min_cluster_size': 3, 'min_samples': 1, 'cluster_selection_method': 'leaf', 'metric': 'manhattan'}\n",
      "Best DBCV score: 0.675\n",
      "Best parameters: {'min_cluster_size': 3, 'min_samples': 5, 'cluster_selection_method': 'leaf', 'metric': 'euclidean'}\n",
      "Best DBCV score: 0.451\n",
      "Best parameters: {'min_cluster_size': 2, 'min_samples': 4, 'cluster_selection_method': 'leaf', 'metric': 'euclidean'}\n",
      "Best DBCV score: 0.271\n",
      "Best parameters: {'min_cluster_size': 2, 'min_samples': 1, 'cluster_selection_method': 'leaf', 'metric': 'euclidean'}\n",
      "Best DBCV score: 0.312\n",
      "Best parameters: {'min_cluster_size': 5, 'min_samples': 3, 'cluster_selection_method': 'leaf', 'metric': 'manhattan'}\n",
      "Best DBCV score: 0.231\n",
      "Best parameters: {'min_cluster_size': 3, 'min_samples': 1, 'cluster_selection_method': 'leaf', 'metric': 'manhattan'}\n",
      "Best DBCV score: 0.960\n",
      "Best parameters: {'min_cluster_size': 4, 'min_samples': 6, 'cluster_selection_method': 'leaf', 'metric': 'manhattan'}\n",
      "Best DBCV score: 0.232\n",
      "Best parameters: {'min_cluster_size': 2, 'min_samples': 2, 'cluster_selection_method': 'leaf', 'metric': 'euclidean'}\n",
      "Best DBCV score: 0.294\n",
      "Best parameters: {'min_cluster_size': 2, 'min_samples': 2, 'cluster_selection_method': 'leaf', 'metric': 'manhattan'}\n",
      "Best DBCV score: 0.355\n",
      "Best parameters: {'min_cluster_size': 6, 'min_samples': 1, 'cluster_selection_method': 'leaf', 'metric': 'euclidean'}\n"
     ]
    }
   ],
   "source": [
    "def create_topic_assigner(start_counter):\n",
    "    counter = start_counter\n",
    "    \n",
    "    def assign_new_topic(x):\n",
    "        nonlocal counter\n",
    "        if x == -1:\n",
    "            new_topic = counter\n",
    "            counter += 1\n",
    "            return new_topic\n",
    "        else:\n",
    "            return x\n",
    "\n",
    "    return assign_new_topic\n",
    "\n",
    "\n",
    "def process_cluster(cluster_df):\n",
    "    # Step 1: Extract embeddings and umap_embeddings\n",
    "    embeddings, doc_titles, docs, ids, umap_embeddings = get_embeddings(cluster_df)\n",
    "\n",
    "    # Step 2: Perform hyperparameter tuning for berttopic\n",
    "    hyperparameters = hyperparameter_tuning(umap_embeddings)\n",
    "\n",
    "    # Step 3: Create and fit topic model \n",
    "    topic_model = topic_modelling(hyperparameters)\n",
    "    topics, _ = topic_model.fit_transform(docs, embeddings)\n",
    "\n",
    "    ###############\n",
    "    # Visualisation \n",
    "    ################\n",
    "    \n",
    "    # Uncomment and adjust as needed for visualization purposes\n",
    "\n",
    "    # top_n = 50\n",
    "    # top_topics = topic_model.get_topic_freq().head(top_n)['Topic'].tolist()\n",
    "\n",
    "    # reduced_embeddings = topic_model.umap_model.embedding_\n",
    "    # hover_data = [f\"{title} - Topic {topic}\" for title, topic in zip(doc_titles, topics)]\n",
    "    # visualization = topic_model.visualize_documents(hover_data, reduced_embeddings=reduced_embeddings, topics=top_topics, title=f'Top {top_n} Topics') \n",
    "    # visualization.show() \n",
    "\n",
    "    # visualization_barchart = topic_model.visualize_barchart(top_n_topics=top_n)\n",
    "    # visualization_barchart.show()\n",
    "\n",
    "    # Step 4: Create a DataFrame with assigned topics, titles and ids.\n",
    "    result_df = pd.DataFrame({\"Assigned Topic\": topics, \"Title\": doc_titles, \"id\": ids})\n",
    "    \n",
    "    # Step 5: Extract topic information and get top 5 keywords, if article is unclustered where Topic is -1, topic representation/kws will be removed\n",
    "    topic_kws = topic_model.get_topic_info()[['Topic', 'Representation']]\n",
    "    topic_kws['top_5_kws'] = topic_kws.apply(lambda row: row['Representation'][:5] if row['Topic'] != -1 else np.nan, axis=1)\n",
    "    \n",
    "    # Step 6: Merge results with the top keywords\n",
    "    result_df_kws = pd.merge(result_df, topic_kws, how='left', left_on='Assigned Topic', right_on='Topic')\n",
    "    result_df_kws = result_df_kws.drop(['Representation', 'Topic'], axis=1)\n",
    "    result_df_kws = result_df_kws[['id', 'Title', 'Assigned Topic', 'top_5_kws']]\n",
    "\n",
    "    # Step 7: Assign new topic numbers to topics that are -1, starting from the max assigned topic in the results_df_kws. \n",
    "    max_topic = result_df_kws['Assigned Topic'].max()\n",
    "    new_topic_counter = max_topic + 1\n",
    "    assign_new_topic_func = create_topic_assigner(new_topic_counter)\n",
    "    result_df_kws['Assigned Topic'] = result_df_kws['Assigned Topic'].apply(assign_new_topic_func)\n",
    "\n",
    "    # Step 8: Update the 'Assigned Topic' column with cluster information to prevent repeat cluster numbers\n",
    "    cluster_id = cluster_df['cluster'].unique()[0]\n",
    "    result_df_kws['Assigned Topic'] = result_df_kws['Assigned Topic'].apply(lambda x: 'Cluster_' + str(cluster_id) + '_' + str(x))\n",
    "\n",
    "    return result_df_kws\n",
    "\n",
    "def process_all_clusters(cluster_morethan10_embeddings):\n",
    "    unique_clusters = cluster_morethan10_embeddings['cluster'].unique()\n",
    "    all_results = []\n",
    "\n",
    "    for cluster_id in unique_clusters:\n",
    "        cluster_df = cluster_morethan10_embeddings[cluster_morethan10_embeddings['cluster'] == cluster_id]\n",
    "        result_df_kws = process_cluster(cluster_df)\n",
    "        all_results.append(result_df_kws)\n",
    "\n",
    "    combined_df = pd.concat(all_results, ignore_index=True)\n",
    "    return combined_df\n",
    "\n",
    "def assign_unique_numbers_to_topics(final_result_df, pred_cluster_df):\n",
    "    \"\"\"\n",
    "    Assigns unique numbers to each unique 'Assigned Topic' in the final_result_df\n",
    "    based on the maximum cluster value from the pred_cluster_df.\n",
    "\n",
    "    Parameters:\n",
    "    final_result_df (pd.DataFrame): DataFrame containing the final results with an 'Assigned Topic' column.\n",
    "    pred_cluster_df (pd.DataFrame): DataFrame containing the predicted clusters with a 'cluster' column.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: Updated final_result_df with an additional 'Assigned Topic Number' column.\n",
    "    \"\"\"\n",
    "    max_cluster_value = pred_cluster_df['cluster'].max()\n",
    "    unique_assigned_topics = final_result_df['Assigned Topic'].unique()\n",
    "    topic_number_mapping = {topic: idx + max_cluster_value + 1 for idx, topic in enumerate(unique_assigned_topics)}\n",
    "    \n",
    "    final_result_df['Assigned Topic Number'] = final_result_df['Assigned Topic'].map(topic_number_mapping)\n",
    "    return final_result_df\n",
    "\n",
    "final_result_df = process_all_clusters(cluster_morethan10_embeddings)\n",
    "final_result_df_with_numbers = assign_unique_numbers_to_topics(final_result_df, pred_cluster_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_cluster_to_merge = final_result_df_with_numbers[['id','top_5_kws','Assigned Topic Number']]\n",
    "new_cluster_to_merge.columns = ['id','cluster_kws','new_cluster']\n",
    "updated_pred_cluster = pd.merge(pred_cluster_df, new_cluster_to_merge, how='left', on='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>url</th>\n",
       "      <th>body_content</th>\n",
       "      <th>cluster</th>\n",
       "      <th>cluster_kws</th>\n",
       "      <th>new_cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1445475</td>\n",
       "      <td>Emotional Suppor...</td>\n",
       "      <td>https://www.heal...</td>\n",
       "      <td>As you may know,...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                title                  url         body_content  \\\n",
       "0  1445475  Emotional Suppor...  https://www.heal...  As you may know,...   \n",
       "\n",
       "   cluster cluster_kws  new_cluster  \n",
       "0        0         NaN            0  "
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updated_pred_cluster['new_cluster'] = updated_pred_cluster['new_cluster'].fillna(updated_pred_cluster['cluster']).apply(int)\n",
    "updated_pred_cluster.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_level_pred_cluster = pd.DataFrame(neo4j_predicted_cluster_pkl)\n",
    "first_level_cluster_dict = dict(zip(first_level_pred_cluster['cluster'], first_level_pred_cluster['cluster_keywords']))\n",
    "\n",
    "mask = updated_pred_cluster['cluster'] == updated_pred_cluster['new_cluster']\n",
    "updated_pred_cluster.loc[mask, 'cluster_kws'] = updated_pred_cluster.loc[mask, 'cluster_kws'].fillna(\n",
    "    updated_pred_cluster['cluster'].map(first_level_cluster_dict)\n",
    ")\n",
    "\n",
    "# Formatting\n",
    "updated_pred_cluster.rename(columns={'cluster':'first_level_cluster','new_cluster':'second_level_cluster','cluster_kws':'second_level_cluster_kws'}, inplace=True)\n",
    "updated_pred_cluster =updated_pred_cluster[['id','title','url','body_content','first_level_cluster','second_level_cluster','second_level_cluster_kws']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_pred_cluster.to_csv(os.path.join(experiment_path,\"predicted_cluster_2nd_level_clustering.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OG_cluster</th>\n",
       "      <th>number_of_articles_in_first_level</th>\n",
       "      <th>number_of_clusters</th>\n",
       "      <th>second_level_cluster_article_counts</th>\n",
       "      <th>number_of_single_articles</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>first_level_cluster</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>74 - ['flu', 'in...</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>[5, 3]</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>117 - ['toddler'...</td>\n",
       "      <td>63</td>\n",
       "      <td>5</td>\n",
       "      <td>[24, 10, 9, 8, 7]</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>171 - ['nutritio...</td>\n",
       "      <td>50</td>\n",
       "      <td>5</td>\n",
       "      <td>[12, 10, 8, 5, 5]</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>195 - ['stress',...</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>[7, 5]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>196 - ['protein'...</td>\n",
       "      <td>35</td>\n",
       "      <td>3</td>\n",
       "      <td>[17, 6, 5]</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>275</th>\n",
       "      <td>275 - ['resilien...</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>[7, 6]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>461</th>\n",
       "      <td>461 - ['aerobic'...</td>\n",
       "      <td>53</td>\n",
       "      <td>2</td>\n",
       "      <td>[37, 16]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>482</th>\n",
       "      <td>482 - ['teeth', ...</td>\n",
       "      <td>21</td>\n",
       "      <td>4</td>\n",
       "      <td>[5, 3, 3, 2]</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>606</th>\n",
       "      <td>606 - ['quit', '...</td>\n",
       "      <td>39</td>\n",
       "      <td>7</td>\n",
       "      <td>[9, 4, 4, 3, 3, ...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>629</th>\n",
       "      <td>629 - ['wholegra...</td>\n",
       "      <td>27</td>\n",
       "      <td>3</td>\n",
       "      <td>[12, 9, 6]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              OG_cluster  number_of_articles_in_first_level  \\\n",
       "first_level_cluster                                                           \n",
       "74                   74 - ['flu', 'in...                   13                 \n",
       "117                  117 - ['toddler'...                   63                 \n",
       "171                  171 - ['nutritio...                   50                 \n",
       "195                  195 - ['stress',...                   12                 \n",
       "196                  196 - ['protein'...                   35                 \n",
       "275                  275 - ['resilien...                   13                 \n",
       "461                  461 - ['aerobic'...                   53                 \n",
       "482                  482 - ['teeth', ...                   21                 \n",
       "606                  606 - ['quit', '...                   39                 \n",
       "629                  629 - ['wholegra...                   27                 \n",
       "\n",
       "                     number_of_clusters second_level_cluster_article_counts  \\\n",
       "first_level_cluster                                                           \n",
       "74                                    2               [5, 3]                  \n",
       "117                                   5    [24, 10, 9, 8, 7]                  \n",
       "171                                   5    [12, 10, 8, 5, 5]                  \n",
       "195                                   2               [7, 5]                  \n",
       "196                                   3           [17, 6, 5]                  \n",
       "275                                   2               [7, 6]                  \n",
       "461                                   2             [37, 16]                  \n",
       "482                                   4         [5, 3, 3, 2]                  \n",
       "606                                   7  [9, 4, 4, 3, 3, ...                  \n",
       "629                                   3           [12, 9, 6]                  \n",
       "\n",
       "                     number_of_single_articles  \n",
       "first_level_cluster                             \n",
       "74                                     5        \n",
       "117                                    5        \n",
       "171                                   10        \n",
       "195                                    0        \n",
       "196                                    7        \n",
       "275                                    0        \n",
       "461                                    0        \n",
       "482                                    8        \n",
       "606                                   11        \n",
       "629                                    0        "
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth',20)\n",
    "updated_pred_cluster = pd.read_csv(os.path.join(experiment_path, \"predicted_cluster_2nd_level_clustering.csv\"))\n",
    "broken_down_groups = updated_pred_cluster[updated_pred_cluster['first_level_cluster'] != updated_pred_cluster['second_level_cluster']]\n",
    "broken_down_groups['second_level_cluster:kws'] = broken_down_groups.apply(lambda x: str(x['second_level_cluster']) + ' : ' + str(x['second_level_cluster_kws']), axis=1)\n",
    "agg_result = broken_down_groups.groupby('first_level_cluster').agg(\n",
    "    number_of_articles_in_first_level = ('first_level_cluster','size'),\n",
    "    number_of_clusters=('second_level_cluster_kws', 'nunique'),\n",
    "    number_of_single_articles=('second_level_cluster_kws', lambda x: x.isna().sum()),\n",
    "    second_level_cluster_article_counts=('second_level_cluster', lambda x: [v for v in x.value_counts().to_dict().values() if v > 1])\n",
    "    # second_level_clusters = ('second_level_cluster:kws',set)\n",
    ")\n",
    "agg_result\n",
    "\n",
    "agg_result[\"OG_keywords\"] = agg_result.index.map(first_level_cluster_dict)\n",
    "agg_result['OG_cluster'] = agg_result.index.astype(str) + ' - ' + agg_result['OG_keywords'].astype(str)\n",
    "agg_result =agg_result[['OG_cluster','number_of_articles_in_first_level','number_of_clusters','second_level_cluster_article_counts','number_of_single_articles']]\n",
    "agg_result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results = []\n",
    "# for group_id in broken_down_groups['first_level_cluster'].unique():\n",
    "#     print('First Level Group ID:',group_id)\n",
    "#     print(first_level_cluster_dict[group_id])\n",
    "#     df = broken_down_groups[broken_down_groups['first_level_cluster'] == group_id]\n",
    "#     print('First Level Group Size: ', df.shape[0])\n",
    "#     mt_1 = df['second_level_cluster'].value_counts() > 1\n",
    "#     eq_1 = df['second_level_cluster'].value_counts() == 1\n",
    "#     print('Single Nodes: ', eq_1.sum())\n",
    "#     display(df[df['second_level_cluster'].isin(eq_1[eq_1].index)].title.to_list())\n",
    "#     grouped_df = df.groupby(['second_level_cluster', 'second_level_cluster_kws']).size()[mt_1[mt_1].index].reset_index(name='No. of Articles')\n",
    "#     display(grouped_df)\n",
    "#     print('-----------')\n",
    "\n",
    "#     for index, row in grouped_df.iterrows():\n",
    "#         results.append([\n",
    "#             group_id, \n",
    "#             first_level_cluster_dict[group_id], \n",
    "#             df.shape[0], \n",
    "#             row['second_level_cluster'], \n",
    "#             row['second_level_cluster_kws'], \n",
    "#             row['No. of Articles']\n",
    "#         ])\n",
    "#     results.append([\n",
    "#             group_id, \n",
    "#             first_level_cluster_dict[group_id], \n",
    "#             df.shape[0], \n",
    "#             \"Single Articles\", \n",
    "#             \"\", \n",
    "#             eq_1.sum()]\n",
    "#         )\n",
    "\n",
    "# result_df = pd.DataFrame(results, columns=['Group ID', 'Group Description', 'First Level Group Size', 'Second Level Cluster', 'Second Level Cluster Keywords', 'No. of Articles'])\n",
    "# result_df.to_csv(os.path.join(experiment_path,'break_down_group_clusters.csv'), index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Cluster size</th>\n",
       "      <th>Num of clusters</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2-10</td>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11-20</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>21-30</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>31-40</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Cluster size  Num of clusters\n",
       "0            1              247\n",
       "1         2-10               81\n",
       "2        11-20                4\n",
       "3        21-30                1\n",
       "4        31-40                1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "334\n"
     ]
    }
   ],
   "source": [
    "def get_cluster_size(pred_cluster):\n",
    "    grouped_counts = pred_cluster.groupby('second_level_cluster').size()\n",
    "    filtered_grouped_counts = grouped_counts[grouped_counts != 1]\n",
    "    single_nodes =  len(grouped_counts[grouped_counts == 1])\n",
    "    bins = range(1, filtered_grouped_counts.max() + 10, 10)\n",
    "    labels = [f\"{i}-{i+9}\" for i in bins[:-1]]\n",
    "    labels[0] = '2-10'\n",
    "    binned_counts = pd.cut(filtered_grouped_counts, bins=bins, labels=labels, right=False)\n",
    "    banded_counts = binned_counts.value_counts().sort_index()\n",
    "    cluster_size = pd.DataFrame(banded_counts).reset_index().rename(columns={'index':\"Cluster size\",'count':\"Num of clusters\"})\n",
    "    new_row = {'Cluster size': '1', 'Num of clusters': single_nodes}  # Customize with your data\n",
    "    cluster_size.loc[-1] = new_row\n",
    "    cluster_size = cluster_size.sort_index().reset_index(drop=True)\n",
    "    return cluster_size\n",
    "\n",
    "clusters_size = get_cluster_size(updated_pred_cluster)\n",
    "display(clusters_size)\n",
    "print(clusters_size['Num of clusters'].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no. of clusters: 87\n",
      "min, max cluster size: 2, 37\n",
      "no. of single nodes: 247\n"
     ]
    }
   ],
   "source": [
    "grouped_counts = updated_pred_cluster.groupby('second_level_cluster').size()\n",
    "filtered_grouped_counts = grouped_counts[grouped_counts != 1]\n",
    "print(f\"no. of clusters: {filtered_grouped_counts.value_counts().sum()}\")\n",
    "print(f\"min, max cluster size: {filtered_grouped_counts.min()}, {filtered_grouped_counts.max()}\")\n",
    "print(f\"no. of single nodes: {len(grouped_counts[grouped_counts == 1])}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update neo4j files and pyvis\n",
    "- Drop all single from neo4j_clustered_df\n",
    "- Update cluster number in neo4j_clustered_df \n",
    "- For all singles, update neo4j_unclustered_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_viz(\n",
    "    clustered_nodes: pd.DataFrame,\n",
    "    unclustered_nodes: pd.DataFrame\n",
    "):\n",
    "    clustered_df = clustered_nodes.copy()\n",
    "    unclustered_df = unclustered_nodes.copy()\n",
    " \n",
    "    visual_graph = pyvis.network.Network(select_menu=True, filter_menu=True)\n",
    " \n",
    "    # Add nodes-nodes pair\n",
    "    for _, row in clustered_df.iterrows():\n",
    "        # Add nodes\n",
    "        visual_graph.add_node(\n",
    "            row[\"node_1_title\"],\n",
    "            label=row[\"node_1_title\"],\n",
    "            title=f\"Predicted group: {row['node_1_pred_cluster']}\\nGroup keywords: {row['node_1_cluster_kws']}\\nTitle: {row['node_1_title']}\",\n",
    "            group=row[\"node_1_pred_cluster\"],\n",
    "        )\n",
    "        visual_graph.add_node(\n",
    "            row[\"node_2_title\"],\n",
    "            label=row[\"node_2_title\"],\n",
    "            title=f\"Predicted group: {row['node_2_pred_cluster']}\\nGroup keywords: {row['node_2_cluster_kws']}\\nTitle: {row['node_2_title']}\",\n",
    "            group=row[\"node_2_pred_cluster\"],\n",
    "        )\n",
    " \n",
    "        # Add edge\n",
    "        visual_graph.add_edge(\n",
    "            row[\"node_1_title\"],\n",
    "            row[\"node_2_title\"],\n",
    "            title=f\"Edge Weight: {row['edge_weight']}\",\n",
    "        )\n",
    "   \n",
    "    # Add solo nodes\n",
    "    for _, row in unclustered_df.iterrows():\n",
    "        visual_graph.add_node(\n",
    "            row[\"node_title\"],\n",
    "            label=row[\"node_title\"],\n",
    "            title=f\"Predicted group: No Community\\nTitle: {row['node_title']}\",\n",
    "        )\n",
    " \n",
    "    return visual_graph.show(experiment_path+\"/neo4j_cluster_viz_updated.html\", notebook=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_single = updated_pred_cluster[updated_pred_cluster['second_level_cluster_kws'].isna()]\n",
    "updated_clustered = updated_pred_cluster[updated_pred_cluster['second_level_cluster_kws'].notna()]\n",
    "\n",
    "update_single_title_list = updated_single.title.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "neo4j_predicted_cluster_csv_path = os.path.join(experiment_path, 'neo_4j_clustered_data.csv')\n",
    "neo4j_unclustered_csv_path = os.path.join(experiment_path, 'neo_4j_unclustered_data.csv')\n",
    "\n",
    "neo4j_clustered_df = pd.read_csv(neo4j_predicted_cluster_csv_path)\n",
    "neo4j_unclustered_df = pd.read_csv(neo4j_unclustered_csv_path)\n",
    "\n",
    "# Remove those that are labelled as single in the second level clustering\n",
    "filtered_clustered_df = neo4j_clustered_df[\n",
    "    ~neo4j_clustered_df.node_1_title.isin(update_single_title_list) &\n",
    "    ~neo4j_clustered_df.node_2_title.isin(update_single_title_list)\n",
    "]\n",
    "\n",
    "adjusted_cluster = updated_pred_cluster[updated_pred_cluster['first_level_cluster'] != updated_pred_cluster['second_level_cluster']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update clustered_df\n",
    "\n",
    "def update_clusters(row, update_dict):\n",
    "    node_1_title = row['node_1_title']\n",
    "    node_2_title = row['node_2_title']\n",
    "\n",
    "    node_1_pred_cluster = update_dict[node_1_title][0] if node_1_title in update_dict else row['node_1_pred_cluster']\n",
    "    node_1_cluster_kws = update_dict[node_1_title][1] if node_1_title in update_dict else row['node_1_cluster_kws']\n",
    "\n",
    "    node_2_pred_cluster = update_dict[node_2_title][0] if node_2_title in update_dict else row['node_2_pred_cluster']\n",
    "    node_2_cluster_kws = update_dict[node_2_title][1] if node_2_title in update_dict else row['node_2_cluster_kws']\n",
    "\n",
    "    return pd.Series([node_1_pred_cluster, node_1_cluster_kws, node_2_pred_cluster, node_2_cluster_kws])\n",
    "\n",
    "# Format - title : (second_level_cluster, second_level_cluster_kws)\n",
    "# Only include those groups that is adjusted\n",
    "updated_clusters_dict = {\n",
    "    title: (second_level_cluster, second_level_cluster_kws)\n",
    "    for title, second_level_cluster, second_level_cluster_kws in zip(\n",
    "        adjusted_cluster['title'],\n",
    "        adjusted_cluster['second_level_cluster'],\n",
    "        adjusted_cluster['second_level_cluster_kws']\n",
    "    )\n",
    "}\n",
    "\n",
    "filtered_clustered_df[['node_1_pred_cluster', 'node_1_cluster_kws', 'node_2_pred_cluster', 'node_2_cluster_kws']] = filtered_clustered_df.apply(\n",
    "    update_clusters, update_dict=updated_clusters_dict, axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "224\n",
      "205\n"
     ]
    }
   ],
   "source": [
    "# Update unclustered_df\n",
    "to_add = updated_single[~updated_single.title.isin(neo4j_unclustered_df.node_title.unique())]\n",
    "to_add = to_add[['title','second_level_cluster']].rename(columns={'title':'node_title','second_level_cluster':'node_community'})\n",
    "neo4j_unclustered_df_updated = pd.concat([neo4j_unclustered_df,to_add]).drop(columns='node_meta_desc')\n",
    "\n",
    "df_node_1 = neo4j_clustered_df[['node_1_title', 'node_1_ground_truth']].rename(columns={'node_1_title': 'title', 'node_1_ground_truth': 'ground_truth'})\n",
    "df_node_2 = neo4j_clustered_df[['node_2_title', 'node_2_ground_truth']].rename(columns={'node_2_title': 'title', 'node_2_ground_truth': 'ground_truth'})\n",
    "combined_node_1_2 = pd.concat([df_node_1, df_node_2]).drop_duplicates()\n",
    "combined_node_1_2_dict = dict(zip(combined_node_1_2['title'], combined_node_1_2['ground_truth']))\n",
    "\n",
    "print(neo4j_unclustered_df_updated.node_ground_truth.isna().sum())\n",
    "neo4j_unclustered_df_updated['node_ground_truth'] = neo4j_unclustered_df_updated['node_title'].map(combined_node_1_2_dict).fillna(neo4j_unclustered_df_updated['node_ground_truth'])\n",
    "print(neo4j_unclustered_df_updated.node_ground_truth.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH_CLUSTERED = os.path.join(experiment_path, 'neo_4j_clustered_data_2nd_level_cluster.csv')\n",
    "OUTPUT_PATH_UNCLUSTERED = os.path.join(experiment_path, 'neo_4j_unclustered_data_2nd_level_cluster.csv')\n",
    "\n",
    "filtered_clustered_df.to_csv(OUTPUT_PATH_CLUSTERED)\n",
    "neo4j_unclustered_df_updated.to_csv(OUTPUT_PATH_UNCLUSTERED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data\\07_model_output\\all-MiniLM-L6-v2\\experiment-5b-weighted-similarities-3_title_7_body/neo4j_cluster_viz_updated.html\n"
     ]
    }
   ],
   "source": [
    "cluster_viz(filtered_clustered_df,neo4j_unclustered_df_updated)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
