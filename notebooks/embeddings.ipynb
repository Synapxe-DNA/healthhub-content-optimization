{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings (Neural Network-based Contextual Embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lyndon/healthhub-content-optimization/.venv/lib/python3.11/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "from alive_progress import alive_bar\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import AutoTokenizer\n",
    "from transformers.models.bert import BertTokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "CONTRIBUTOR: str = \"Health Promotion Board\"\n",
    "CATEGORY: str = \"live-healthy\"\n",
    "MODEL_NAME: str = \"all-MiniLM-L6-v2\"\n",
    "POOLING_STRATEGY: str = \"max\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLEAN_DATA_PATH = os.path.join(\"..\", \"data\", \"healthhub_small_clean\")\n",
    "\n",
    "CLEANED_CHUNK_ID_LIST_PATH = os.path.join(\n",
    "    CLEAN_DATA_PATH, \"healthhub_chunk_id_list_small_clean.pkl\"\n",
    ")\n",
    "CLEANED_SOURCE_LIST_PATH = os.path.join(\n",
    "    CLEAN_DATA_PATH, \"healthhub_source_list_small_clean.pkl\"\n",
    ")\n",
    "CLEANED_DOMAIN_LIST_PATH = os.path.join(\n",
    "    CLEAN_DATA_PATH, \"healthhub_domain_list_small_clean.pkl\"\n",
    ")\n",
    "CLEANED_TITLE_LIST_PATH = os.path.join(\n",
    "    CLEAN_DATA_PATH, \"healthhub_title_list_small_clean.pkl\"\n",
    ")\n",
    "CLEANED_CONTRIBUTOR_LIST_PATH = os.path.join(\n",
    "    CLEAN_DATA_PATH, \"healthhub_contributor_list_small_clean.pkl\"\n",
    ")\n",
    "CLEANED_CONTENT_LIST_PATH = os.path.join(\n",
    "    CLEAN_DATA_PATH, \"healthhub_content_list_small_clean.pkl\"\n",
    ")\n",
    "CLEANED_CATEGORY_LIST_PATH = os.path.join(\n",
    "    CLEAN_DATA_PATH, \"healthhub_category_list_small_clean.pkl\"\n",
    ")\n",
    "CLEANED_EMBEDDING_LIST_PATH = os.path.join(\n",
    "    CLEAN_DATA_PATH,\n",
    "    f\"healthhub_{MODEL_NAME.replace('/','_')}_{POOLING_STRATEGY}_embeddings_small_clean.parquet\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(CLEANED_CHUNK_ID_LIST_PATH, \"rb\") as file:\n",
    "    loaded_chunk_id = pickle.load(file)  # list of chunk ids\n",
    "\n",
    "with open(CLEANED_SOURCE_LIST_PATH, \"rb\") as file:\n",
    "    loaded_source = pickle.load(file)  # list of hyperlinks\n",
    "\n",
    "with open(CLEANED_DOMAIN_LIST_PATH, \"rb\") as file:\n",
    "    loaded_domain = pickle.load(file)  # website domain\n",
    "\n",
    "with open(CLEANED_TITLE_LIST_PATH, \"rb\") as file:\n",
    "    loaded_title = pickle.load(file)  # list of titles each chunk belongs to\n",
    "\n",
    "with open(CLEANED_CONTRIBUTOR_LIST_PATH, \"rb\") as file:\n",
    "    loaded_contributor = pickle.load(file)  # list of contributors\n",
    "\n",
    "with open(CLEANED_CONTENT_LIST_PATH, \"rb\") as file:\n",
    "    loaded_content = pickle.load(file)  # list of chunks of contents\n",
    "\n",
    "with open(CLEANED_CATEGORY_LIST_PATH, \"rb\") as file:\n",
    "    loaded_category = pickle.load(file)  # list of categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(\n",
    "    {\n",
    "        \"chunk_id\": loaded_chunk_id,\n",
    "        \"doc_source\": loaded_source,\n",
    "        \"doc_domain\": loaded_domain,\n",
    "        \"doc_title\": loaded_title,\n",
    "        \"contributor\": loaded_contributor,\n",
    "        \"text\": loaded_content,\n",
    "        \"category\": loaded_category,\n",
    "    }\n",
    ")\n",
    "\n",
    "df = df[df[\"contributor\"] == CONTRIBUTOR].reset_index(drop=True)\n",
    "df = df[df[\"doc_source\"].apply(lambda x: x.split(\"/\")[3] == CATEGORY)].reset_index(\n",
    "    drop=True\n",
    ")\n",
    "\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    len(loaded_content),\n",
    "    len(loaded_chunk_id),\n",
    "    len(loaded_source),\n",
    "    len(loaded_domain),\n",
    "    len(loaded_title),\n",
    "    len(loaded_contributor),\n",
    "    len(loaded_category),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"combined_text\"] = None\n",
    "\n",
    "with alive_bar(df[\"doc_source\"].nunique(), force_tty=True) as bar:\n",
    "    for source in df[\"doc_source\"].unique():\n",
    "        combined_text = \" \".join(df.query(\"doc_source == @source\")[\"text\"].values)\n",
    "        indices = df.query(\"doc_source == @source\").index.values\n",
    "        df.loc[indices, \"combined_text\"] = combined_text\n",
    "        bar()\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizer and model\n",
    "sentence_transformer = SentenceTransformer(MODEL_NAME)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "max_length = sentence_transformer.max_seq_length  # 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_into_chunks(\n",
    "    sentences: list[str], max_length: int, tokenizer: BertTokenizerFast\n",
    ") -> list[str]:\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_length = 0\n",
    "\n",
    "    for sentence in sentences:\n",
    "        # Tokenize the sentence\n",
    "        encoded_sentence = tokenizer(sentence, return_tensors=\"pt\")\n",
    "        num_tokens = encoded_sentence[\"input_ids\"].shape[1]\n",
    "\n",
    "        # If adding the current sentence would exceed max_length, save the current chunk and start a new one\n",
    "        if current_length + num_tokens > max_length:\n",
    "            chunks.append(\" \".join(current_chunk))\n",
    "            current_chunk = []\n",
    "            current_length = 0\n",
    "\n",
    "        current_chunk.append(sentence)\n",
    "        current_length += num_tokens\n",
    "\n",
    "    # Add the last chunk if any\n",
    "    if current_chunk:\n",
    "        chunks.append(\" \".join(current_chunk))\n",
    "\n",
    "    return chunks\n",
    "\n",
    "\n",
    "def pool_embeddings(embeddings: np.ndarray, strategy: str = \"mean\") -> np.ndarray:\n",
    "    if not embeddings:\n",
    "        raise ValueError(\"The embeddings are empty.\")\n",
    "\n",
    "    if strategy == \"mean\":\n",
    "        return np.mean(embeddings, axis=0)\n",
    "    elif strategy == \"max\":\n",
    "        return np.max(embeddings, axis=0)\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            \"Pooling strategy not recognized. The strategy must be either 'average' or 'max'.\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_embeddings = []\n",
    "\n",
    "with alive_bar(df[\"doc_source\"].nunique(), force_tty=True) as bar:\n",
    "    for source in df[\"doc_source\"].unique():\n",
    "        combined_text = df.query(\"doc_source == @source\")[\"combined_text\"].values[0]\n",
    "\n",
    "        # Step 1: Split the article into sentences\n",
    "        sentences = sent_tokenize(combined_text)\n",
    "\n",
    "        # Step 2: Tokenize sentences and split into chunks of max 256 tokens\n",
    "        chunks = split_into_chunks(sentences, max_length, tokenizer)\n",
    "\n",
    "        # Step 3: Encode each chunk to get their embeddings\n",
    "        chunk_embeddings = [sentence_transformer.encode(chunk) for chunk in chunks]\n",
    "\n",
    "        # Step 4: Aggregate chunk embeddings to form a single embedding for the entire article\n",
    "        article_embedding = pool_embeddings(chunk_embeddings, strategy=POOLING_STRATEGY)\n",
    "\n",
    "        indices = df.query(\"doc_source == @source\").index.values\n",
    "\n",
    "        for _ in range(len(indices)):\n",
    "            article_embeddings.append(article_embedding)\n",
    "\n",
    "        bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_col = f\"{MODEL_NAME}_{POOLING_STRATEGY}_embeddings\"\n",
    "\n",
    "df[embedding_col] = article_embeddings\n",
    "df = df[~df[\"doc_source\"].duplicated()].reset_index(drop=True)\n",
    "df[\"chunk_id\"] = df[\"chunk_id\"].apply(lambda x: \"_\".join(x.split(\"_\")[:-1]))\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = pa.Table.from_pandas(df)\n",
    "pq.write_table(table, CLEANED_EMBEDDING_LIST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mining",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
