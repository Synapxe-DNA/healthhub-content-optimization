{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings (Traditional Statistical Vector-based Embeddings)\n",
    "\n",
    "Yes, traditional statistical vector-based embeddings are foundational techniques in natural language processing (NLP) that represent text data using various statistical measures. Here are some of these traditional methods:\n",
    "\n",
    "### 1. Bag of Words (BoW)\n",
    "- **Description**: Represents text by the occurrence (count) of each word in the document without considering the word order or context.\n",
    "- **Implementation**: Typically uses a Count Vectorizer.\n",
    "- **Characteristics**: Produces sparse vectors where each dimension corresponds to a specific term from the vocabulary and the value is the word count.\n",
    "- **Use Cases**: Simple and effective for basic text classification and clustering tasks.\n",
    "\n",
    "### 2. Term Frequency-Inverse Document Frequency (TF-IDF)\n",
    "- **Description**: Enhances the Bag of Words model by weighting terms based on their frequency in a document and their inverse frequency across all documents in the corpus.\n",
    "- **Implementation**: Uses TF-IDF Vectorizer.\n",
    "- **Characteristics**: Produces sparse vectors with weighted values, reducing the impact of common words and highlighting important terms.\n",
    "- **Use Cases**: Widely used in information retrieval and text mining.\n",
    "\n",
    "### 3. Latent Semantic Analysis (LSA) or Latent Semantic Indexing (LSI)\n",
    "- **Description**: Applies Singular Value Decomposition (SVD) to the term-document matrix (typically after applying TF-IDF) to reduce dimensions and capture latent semantic relationships between terms.\n",
    "- **Implementation**: Perform SVD on the term-document matrix.\n",
    "- **Characteristics**: Transforms high-dimensional sparse vectors into lower-dimensional dense vectors.\n",
    "- **Use Cases**: Useful for topic modeling and capturing underlying semantic structures.\n",
    "\n",
    "### 4. Latent Dirichlet Allocation (LDA)\n",
    "- **Description**: A generative probabilistic model that represents documents as mixtures of topics and topics as mixtures of words.\n",
    "- **Implementation**: Uses probabilistic algorithms to infer topic distributions.\n",
    "- **Characteristics**: Produces dense vectors representing the distribution of topics in each document.\n",
    "- **Use Cases**: Widely used for topic modeling and discovering abstract topics in large text corpora.\n",
    "\n",
    "### 5. Pointwise Mutual Information (PMI)\n",
    "- **Description**: Measures the association between a pair of words by comparing the probability of their co-occurrence to the probabilities of their individual occurrences.\n",
    "- **Implementation**: Uses co-occurrence matrices.\n",
    "- **Characteristics**: Produces dense vectors that capture the likelihood of words appearing together.\n",
    "- **Use Cases**: Useful for capturing word associations and semantic relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from alive_progress import alive_bar\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.decomposition import LatentDirichletAllocation, TruncatedSVD\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import (\n",
    "    cosine_similarity,\n",
    "    euclidean_distances,\n",
    "    manhattan_distances,\n",
    ")\n",
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statistical_methods = {\n",
    "    \"bow\": (CountVectorizer,),\n",
    "    \"tfidf\": (TfidfVectorizer,),\n",
    "    \"lsa\": (TfidfVectorizer, TruncatedSVD),\n",
    "    \"lda\": (CountVectorizer, LatentDirichletAllocation),\n",
    "}\n",
    "\n",
    "similarity_metrics = {\n",
    "    \"cosine\": cosine_similarity,\n",
    "    \"euclidean\": euclidean_distances,\n",
    "    \"dot\": np.dot,\n",
    "    \"manhattan\": manhattan_distances,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "CONTRIBUTOR: str = \"Health Promotion Board\"\n",
    "CATEGORY: str = \"live-healthy\"\n",
    "METHOD: str = \"bow\"\n",
    "KWARGS: dict = {\"max_features\": 384}\n",
    "METRIC: str = \"dot\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLEAN_DATA_PATH = os.path.join(\"..\", \"data\", \"healthhub_small_clean\")\n",
    "\n",
    "CLEANED_CHUNK_ID_LIST_PATH = os.path.join(\n",
    "    CLEAN_DATA_PATH, \"healthhub_chunk_id_list_small_clean.pkl\"\n",
    ")\n",
    "CLEANED_SOURCE_LIST_PATH = os.path.join(\n",
    "    CLEAN_DATA_PATH, \"healthhub_source_list_small_clean.pkl\"\n",
    ")\n",
    "CLEANED_DOMAIN_LIST_PATH = os.path.join(\n",
    "    CLEAN_DATA_PATH, \"healthhub_domain_list_small_clean.pkl\"\n",
    ")\n",
    "CLEANED_TITLE_LIST_PATH = os.path.join(\n",
    "    CLEAN_DATA_PATH, \"healthhub_title_list_small_clean.pkl\"\n",
    ")\n",
    "CLEANED_CONTRIBUTOR_LIST_PATH = os.path.join(\n",
    "    CLEAN_DATA_PATH, \"healthhub_contributor_list_small_clean.pkl\"\n",
    ")\n",
    "CLEANED_CONTENT_LIST_PATH = os.path.join(\n",
    "    CLEAN_DATA_PATH, \"healthhub_content_list_small_clean.pkl\"\n",
    ")\n",
    "CLEANED_CATEGORY_LIST_PATH = os.path.join(\n",
    "    CLEAN_DATA_PATH, \"healthhub_category_list_small_clean.pkl\"\n",
    ")\n",
    "\n",
    "OUTPUT_CM_PATH = os.path.join(\n",
    "    \"..\",\n",
    "    \"artifacts\",\n",
    "    \"outputs\",\n",
    "    f\"{METHOD}_{'_'.join([f'{k}_{v}' for k, v in KWARGS.items()])}_{METRIC}_cm.png\",\n",
    ")\n",
    "OUTPUT_SIM_PATH = os.path.join(\n",
    "    \"..\",\n",
    "    \"artifacts\",\n",
    "    \"outputs\",\n",
    "    \"statistical_vector_based_embeddings_similarity_scores.xlsx\",\n",
    ")\n",
    "\n",
    "SHEET_NAME = f\"{METHOD}_{METRIC}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(CLEANED_CHUNK_ID_LIST_PATH, \"rb\") as file:\n",
    "    loaded_chunk_id = pickle.load(file)  # list of chunk ids\n",
    "\n",
    "with open(CLEANED_SOURCE_LIST_PATH, \"rb\") as file:\n",
    "    loaded_source = pickle.load(file)  # list of hyperlinks\n",
    "\n",
    "with open(CLEANED_DOMAIN_LIST_PATH, \"rb\") as file:\n",
    "    loaded_domain = pickle.load(file)  # website domain\n",
    "\n",
    "with open(CLEANED_TITLE_LIST_PATH, \"rb\") as file:\n",
    "    loaded_title = pickle.load(file)  # list of titles each chunk belongs to\n",
    "\n",
    "with open(CLEANED_CONTRIBUTOR_LIST_PATH, \"rb\") as file:\n",
    "    loaded_contributor = pickle.load(file)  # list of contributors\n",
    "\n",
    "with open(CLEANED_CONTENT_LIST_PATH, \"rb\") as file:\n",
    "    loaded_content = pickle.load(file)  # list of chunks of contents\n",
    "\n",
    "with open(CLEANED_CATEGORY_LIST_PATH, \"rb\") as file:\n",
    "    loaded_category = pickle.load(file)  # list of categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(\n",
    "    {\n",
    "        \"chunk_id\": loaded_chunk_id,\n",
    "        \"doc_source\": loaded_source,\n",
    "        \"doc_domain\": loaded_domain,\n",
    "        \"doc_title\": loaded_title,\n",
    "        \"contributor\": loaded_contributor,\n",
    "        \"text\": loaded_content,\n",
    "        \"category\": loaded_category,\n",
    "    }\n",
    ")\n",
    "\n",
    "df = df[df[\"contributor\"] == CONTRIBUTOR].reset_index(drop=True)\n",
    "df = df[df[\"doc_source\"].apply(lambda x: x.split(\"/\")[3] == CATEGORY)].reset_index(\n",
    "    drop=True\n",
    ")\n",
    "\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine Chunks into Single Articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"combined_text\"] = None\n",
    "\n",
    "with alive_bar(df[\"doc_source\"].nunique(), force_tty=True) as bar:\n",
    "    for source in df[\"doc_source\"].unique():\n",
    "        combined_text = \" \".join(df.query(\"doc_source == @source\")[\"text\"].values)\n",
    "        indices = df.query(\"doc_source == @source\").index.values\n",
    "        df.loc[indices, \"combined_text\"] = combined_text\n",
    "        bar()\n",
    "\n",
    "# After combining chunks one article, remove all duplicate articles\n",
    "df = df[~df[\"doc_source\"].duplicated()].reset_index(drop=True)\n",
    "df[\"chunk_id\"] = df[\"chunk_id\"].apply(lambda x: \"_\".join(x.split(\"_\")[:-1]))\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Ground Truth Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_df = pd.read_excel(\n",
    "    os.path.join(\n",
    "        \"..\", \"data\", \"Synapxe Content Prioritisation - Live Healthy_020724.xlsx\"\n",
    "    ),\n",
    "    sheet_name=\"All Live Healthy\",\n",
    "    index_col=False,\n",
    ")\n",
    "\n",
    "ground_truth_col = \"Combine Group ID\"\n",
    "\n",
    "ground_df = ground_df[ground_df[ground_truth_col].notna()].reset_index(drop=True)\n",
    "ground_df[ground_truth_col] = ground_df[ground_truth_col].astype(int)\n",
    "\n",
    "# Merge dfs so we can get the document title and content\n",
    "merge_df = pd.merge(ground_df, df, how=\"inner\", left_on=\"URL\", right_on=\"doc_source\")\n",
    "\n",
    "col_of_int = [\"Combine Group ID\", \"Page Title\", \"Meta Description\", *df.columns]\n",
    "final_df = merge_df[col_of_int]\n",
    "\n",
    "print(final_df.shape)\n",
    "final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_statistical_embeddings(\n",
    "    corpus: list[str], method: str, **kwargs: dict\n",
    ") -> tuple[csr_matrix, pd.DataFrame] | tuple[np.ndarray, None]:\n",
    "    components = statistical_methods.get(method, None)\n",
    "\n",
    "    df = None\n",
    "\n",
    "    if len(components) == 1 and components is not None:\n",
    "        vectorizer = components[0](**kwargs)\n",
    "        print(vectorizer)\n",
    "        X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "        # Get words from stopwords array to use as headers\n",
    "        feature_names = vectorizer.get_feature_names_out()\n",
    "        # Combine header titles and weights\n",
    "        df = pd.DataFrame(X.toarray(), columns=feature_names)\n",
    "\n",
    "    elif len(components) > 1 and components is not None:\n",
    "        pipeline = make_pipeline(components[0](), components[1](**kwargs))\n",
    "        print(pipeline)\n",
    "        X = pipeline.fit_transform(corpus)\n",
    "\n",
    "    return X, df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, mat_df = generate_statistical_embeddings(\n",
    "    final_df[\"combined_text\"].to_list(), method=METHOD, **KWARGS\n",
    ")\n",
    "\n",
    "print(X.shape)  # (num_docs, emb_dim)\n",
    "if mat_df is not None:\n",
    "    display(mat_df.head(7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute similarity matrix\n",
    "similarity_metric = similarity_metrics[METRIC]\n",
    "\n",
    "if METRIC == \"dot\":\n",
    "    similarities = X @ X.T\n",
    "    if type(similarities) != np.ndarray:\n",
    "        similarities = similarities.toarray()\n",
    "elif METRIC in [\"euclidean\", \"manhattan\"]:\n",
    "    distances = similarity_metric(X, X)\n",
    "    # https://stats.stackexchange.com/questions/158279/how-i-can-convert-distance-euclidean-to-similarity-score#:~:text=If,is%20commonly%20used.\n",
    "    similarities = 1 / (1 + distances)\n",
    "else:\n",
    "    similarities = similarity_metric(X, X)\n",
    "\n",
    "print(similarities.shape)  # (num_docs, num_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if METHOD == \"bow\" and METRIC == \"dot\":\n",
    "    similarities = np.divide(similarities, similarities.max(), casting=\"same_kind\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to darken a hex color\n",
    "\n",
    "\n",
    "def darken_hex_color(hex_color, factor=0.7):\n",
    "    # Ensure factor is between 0 and 1\n",
    "    factor = max(0, min(1, factor))\n",
    "\n",
    "    # Convert hex color to RGB\n",
    "    r = int(hex_color[1:3], 16)\n",
    "    g = int(hex_color[3:5], 16)\n",
    "    b = int(hex_color[5:7], 16)\n",
    "\n",
    "    # Darken the color\n",
    "    r = int(r * factor)\n",
    "    g = int(g * factor)\n",
    "    b = int(b * factor)\n",
    "\n",
    "    # Convert RGB back to hex\n",
    "    darkened_color = f\"#{r:02x}{g:02x}{b:02x}\".upper()\n",
    "\n",
    "    return darkened_color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_titles = final_df.loc[:, \"doc_title\"].tolist()\n",
    "\n",
    "start = 0\n",
    "end = 20\n",
    "\n",
    "cutoff_similarities = similarities[start:end, start:end]\n",
    "cutoff_article_titles = article_titles[start:end]\n",
    "\n",
    "# Generate random colours\n",
    "hexadecimal_alphabets = \"0123456789ABCDEF\"\n",
    "ground_truth_cluster_ids = final_df.iloc[start:end][\"Combine Group ID\"].unique()\n",
    "colours = {\n",
    "    id: darken_hex_color(\n",
    "        \"#\" + \"\".join([random.choice(hexadecimal_alphabets) for _ in range(6)])\n",
    "    )\n",
    "    for id in ground_truth_cluster_ids\n",
    "}\n",
    "\n",
    "\n",
    "plt.subplots(figsize=(20, 18))\n",
    "ax = sns.heatmap(\n",
    "    cutoff_similarities,\n",
    "    xticklabels=cutoff_article_titles,\n",
    "    yticklabels=cutoff_article_titles,\n",
    "    annot=True,\n",
    "    fmt=\".2g\",\n",
    ")\n",
    "\n",
    "for x_tick_label, y_tick_label in zip(\n",
    "    ax.axes.get_xticklabels(), ax.axes.get_yticklabels()\n",
    "):\n",
    "    ground_truth_cluster_id = (\n",
    "        final_df[final_df[\"doc_title\"] == y_tick_label.get_text()][\"Combine Group ID\"]\n",
    "        .values[0]\n",
    "        .astype(int)\n",
    "    )\n",
    "    colour = colours[ground_truth_cluster_id]\n",
    "    y_tick_label.set_color(colour)\n",
    "    x_tick_label.set_color(colour)\n",
    "\n",
    "ax.set_title(f\"Method: {METHOD}, kwargs: {KWARGS}, metric: {METRIC}\", fontsize=20)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax.figure.savefig(OUTPUT_CM_PATH, dpi=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Similarity Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_df = pd.DataFrame(similarities)\n",
    "\n",
    "sim_df.index = merge_df[\"Page Title\"]\n",
    "sim_df.columns = merge_df[\"Page Title\"]\n",
    "\n",
    "# Store kwwargs as index name\n",
    "sim_df.index.name = str(KWARGS)\n",
    "sim_df.columns.name = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isfile(OUTPUT_SIM_PATH):\n",
    "    with pd.ExcelWriter(\n",
    "        OUTPUT_SIM_PATH, mode=\"a\", engine=\"openpyxl\", if_sheet_exists=\"replace\"\n",
    "    ) as writer:  # Open with pd.ExcelWriter\n",
    "        sim_df.to_excel(writer, sheet_name=SHEET_NAME)\n",
    "else:\n",
    "    sim_df.to_excel(OUTPUT_SIM_PATH, sheet_name=SHEET_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = pd.ExcelFile(OUTPUT_SIM_PATH)\n",
    "print(file.sheet_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = pd.read_excel(OUTPUT_SIM_PATH, sheet_name=SHEET_NAME)\n",
    "print(ast.literal_eval(tmp.columns[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mining",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
