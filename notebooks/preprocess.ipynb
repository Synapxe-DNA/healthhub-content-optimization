{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = os.path.join(\"..\", \"data\", \"healthhub_small\")\n",
    "CLEAN_DATA_PATH = os.path.join(\"..\", \"data\", \"healthhub_small_clean\")\n",
    "\n",
    "LOADED_CHUNK_ID_LIST_PATH = os.path.join(DATA_PATH, \"healthhub_chunk_id_list_small.pkl\")\n",
    "LOADED_SOURCE_LIST_PATH = os.path.join(DATA_PATH, \"healthhub_source_list_small.pkl\")\n",
    "LOADED_DOMAIN_LIST_PATH = os.path.join(DATA_PATH, \"healthhub_domain_list_small.pkl\")\n",
    "LOADED_TITLE_LIST_PATH = os.path.join(DATA_PATH, \"healthhub_title_list_small.pkl\")\n",
    "LOADED_CONTRIBUTOR_LIST_PATH = os.path.join(\n",
    "    DATA_PATH, \"healthhub_contributor_list_small.pkl\"\n",
    ")\n",
    "LOADED_CONTENT_LIST_PATH = os.path.join(DATA_PATH, \"healthhub_content_list_small.pkl\")\n",
    "\n",
    "CLEANED_CHUNK_ID_LIST_PATH = os.path.join(\n",
    "    CLEAN_DATA_PATH, \"healthhub_chunk_id_list_small_clean.pkl\"\n",
    ")\n",
    "CLEANED_SOURCE_LIST_PATH = os.path.join(\n",
    "    CLEAN_DATA_PATH, \"healthhub_source_list_small_clean.pkl\"\n",
    ")\n",
    "CLEANED_DOMAIN_LIST_PATH = os.path.join(\n",
    "    CLEAN_DATA_PATH, \"healthhub_domain_list_small_clean.pkl\"\n",
    ")\n",
    "CLEANED_TITLE_LIST_PATH = os.path.join(\n",
    "    CLEAN_DATA_PATH, \"healthhub_title_list_small_clean.pkl\"\n",
    ")\n",
    "CLEANED_CONTRIBUTOR_LIST_PATH = os.path.join(\n",
    "    CLEAN_DATA_PATH, \"healthhub_contributor_list_small_clean.pkl\"\n",
    ")\n",
    "CLEANED_CONTENT_LIST_PATH = os.path.join(\n",
    "    CLEAN_DATA_PATH, \"healthhub_content_list_small_clean.pkl\"\n",
    ")\n",
    "CLEANED_CATEGORY_LIST_PATH = os.path.join(\n",
    "    CLEAN_DATA_PATH, \"healthhub_category_list_small_clean.pkl\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(CLEAN_DATA_PATH, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(LOADED_CHUNK_ID_LIST_PATH, \"rb\") as file:\n",
    "    loaded_chunk_id = pickle.load(file)  # list of chunk ids\n",
    "\n",
    "with open(LOADED_SOURCE_LIST_PATH, \"rb\") as file:\n",
    "    loaded_source = pickle.load(file)  # list of hyperlinks\n",
    "\n",
    "with open(LOADED_DOMAIN_LIST_PATH, \"rb\") as file:\n",
    "    loaded_domain = pickle.load(file)  # website domain\n",
    "\n",
    "with open(LOADED_TITLE_LIST_PATH, \"rb\") as file:\n",
    "    loaded_title = pickle.load(file)  # list of titles each chunk belongs to\n",
    "\n",
    "with open(LOADED_CONTRIBUTOR_LIST_PATH, \"rb\") as file:\n",
    "    loaded_contributor = pickle.load(file)  # list of contributors\n",
    "\n",
    "with open(LOADED_CONTENT_LIST_PATH, \"rb\") as file:\n",
    "    loaded_content = pickle.load(file)  # list of chunks of contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7121, 6)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chunk_id</th>\n",
       "      <th>doc_source</th>\n",
       "      <th>doc_domain</th>\n",
       "      <th>doc_title</th>\n",
       "      <th>contributor</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>web_crawl_76e2f466-d0b1-58bb-a697-84f5569cd801_1</td>\n",
       "      <td>https://www.healthhub.sg/a-z/medications/proch...</td>\n",
       "      <td>healthhub.sg</td>\n",
       "      <td>prochlorperazine</td>\n",
       "      <td>Pharmaceutical Society of Singapore</td>\n",
       "      <td>HOME\\n\\r\\n A-Z\\r\\n \\n A\\n A\\n A\\nProchlorperaz...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>web_crawl_76e2f466-d0b1-58bb-a697-84f5569cd801_2</td>\n",
       "      <td>https://www.healthhub.sg/a-z/medications/proch...</td>\n",
       "      <td>healthhub.sg</td>\n",
       "      <td>prochlorperazine</td>\n",
       "      <td>Pharmaceutical Society of Singapore</td>\n",
       "      <td>What precautions should I take?\\nInform your h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>web_crawl_76e2f466-d0b1-58bb-a697-84f5569cd801_3</td>\n",
       "      <td>https://www.healthhub.sg/a-z/medications/proch...</td>\n",
       "      <td>healthhub.sg</td>\n",
       "      <td>prochlorperazine</td>\n",
       "      <td>Pharmaceutical Society of Singapore</td>\n",
       "      <td>Avoid drinking alcohol while taking this medic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>web_crawl_76e2f466-d0b1-58bb-a697-84f5569cd801_4</td>\n",
       "      <td>https://www.healthhub.sg/a-z/medications/proch...</td>\n",
       "      <td>healthhub.sg</td>\n",
       "      <td>prochlorperazine</td>\n",
       "      <td>Pharmaceutical Society of Singapore</td>\n",
       "      <td>Keep this medication away from children.\\nHow ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>web_crawl_7c5274b5-4c19-57c2-a5b6-08fc77cf9189_1</td>\n",
       "      <td>https://www.healthhub.sg/programmes/parent-hub...</td>\n",
       "      <td>healthhub.sg</td>\n",
       "      <td>positive-parenting-programme</td>\n",
       "      <td>Health Promotion Board</td>\n",
       "      <td>HOME\\n\\r\\n PROGRAMMES\\r\\n \\n A\\n A\\n A\\nParent...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           chunk_id  \\\n",
       "0  web_crawl_76e2f466-d0b1-58bb-a697-84f5569cd801_1   \n",
       "1  web_crawl_76e2f466-d0b1-58bb-a697-84f5569cd801_2   \n",
       "2  web_crawl_76e2f466-d0b1-58bb-a697-84f5569cd801_3   \n",
       "3  web_crawl_76e2f466-d0b1-58bb-a697-84f5569cd801_4   \n",
       "4  web_crawl_7c5274b5-4c19-57c2-a5b6-08fc77cf9189_1   \n",
       "\n",
       "                                          doc_source    doc_domain  \\\n",
       "0  https://www.healthhub.sg/a-z/medications/proch...  healthhub.sg   \n",
       "1  https://www.healthhub.sg/a-z/medications/proch...  healthhub.sg   \n",
       "2  https://www.healthhub.sg/a-z/medications/proch...  healthhub.sg   \n",
       "3  https://www.healthhub.sg/a-z/medications/proch...  healthhub.sg   \n",
       "4  https://www.healthhub.sg/programmes/parent-hub...  healthhub.sg   \n",
       "\n",
       "                      doc_title                          contributor  \\\n",
       "0              prochlorperazine  Pharmaceutical Society of Singapore   \n",
       "1              prochlorperazine  Pharmaceutical Society of Singapore   \n",
       "2              prochlorperazine  Pharmaceutical Society of Singapore   \n",
       "3              prochlorperazine  Pharmaceutical Society of Singapore   \n",
       "4  positive-parenting-programme               Health Promotion Board   \n",
       "\n",
       "                                                text  \n",
       "0  HOME\\n\\r\\n A-Z\\r\\n \\n A\\n A\\n A\\nProchlorperaz...  \n",
       "1  What precautions should I take?\\nInform your h...  \n",
       "2  Avoid drinking alcohol while taking this medic...  \n",
       "3  Keep this medication away from children.\\nHow ...  \n",
       "4  HOME\\n\\r\\n PROGRAMMES\\r\\n \\n A\\n A\\n A\\nParent...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(\n",
    "    {\n",
    "        \"chunk_id\": loaded_chunk_id,\n",
    "        \"doc_source\": loaded_source,\n",
    "        \"doc_domain\": loaded_domain,\n",
    "        \"doc_title\": loaded_title,\n",
    "        \"contributor\": loaded_contributor,\n",
    "        \"text\": loaded_content,\n",
    "    }\n",
    ")\n",
    "\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove Duplicated chunks (same content but different links due to capitalisation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"lowercase_doc_source\"] = df.doc_source.str.lower()\n",
    "\n",
    "# Group by the lowercased `doc_source` and find the number of unique `doc_source`\n",
    "# Note: If number of unique `doc_source` > 1, then there is/are a duplicate(s)\n",
    "dup_df = (\n",
    "    df.groupby(\"lowercase_doc_source\")[\"doc_source\"].apply(set).apply(len).reset_index()\n",
    ")\n",
    "\n",
    "# For all those `doc_source` > 1, we get the unique `lowercase_doc_source`\n",
    "unique_dup_doc = dup_df[dup_df[\"doc_source\"] > 1][\"lowercase_doc_source\"].unique()\n",
    "\n",
    "# Get index of data which `doc_source` does not match `lowercase_doc_source` + in `unique_dup_doc`\n",
    "to_drop_indices = df[\n",
    "    (df[\"doc_source\"] != df[\"lowercase_doc_source\"])\n",
    "    & (df[\"lowercase_doc_source\"].isin(unique_dup_doc))\n",
    "].index\n",
    "\n",
    "print(f\"Number of rows to drop: {len(to_drop_indices)}\")\n",
    "\n",
    "# Filter only those rows which are not in `to_drop_indices`\n",
    "df = df[~df.index.isin(to_drop_indices)].reset_index(drop=True)\n",
    "\n",
    "print(df.shape)\n",
    "\n",
    "print(f'Number of rows to drop: {df.duplicated(subset=[\"text\", \"doc_source\"]).sum()}')\n",
    "\n",
    "# Remove completely same text and doc_source\n",
    "df = df.drop_duplicates(subset=[\"text\", \"doc_source\"]).reset_index(drop=True)\n",
    "\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"doc_source\"] = df[\"lowercase_doc_source\"]\n",
    "df = df.drop(\"lowercase_doc_source\", axis=1)\n",
    "\n",
    "df[\"category\"] = df[\"doc_source\"].apply(lambda x: x.split(\"/\")[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(text):\n",
    "    def remove_after_phrase(text, phrase):\n",
    "        phrase_index = text.find(phrase)\n",
    "        if phrase_index != -1:\n",
    "            return text[:phrase_index]\n",
    "        return text\n",
    "\n",
    "    def remove_before_phrase(text, phrase):\n",
    "        phrase_index = text.find(phrase)\n",
    "        if phrase_index != -1:\n",
    "            return text[phrase_index + len(phrase) :]\n",
    "        return text\n",
    "\n",
    "    # Remove text \"Read these next\" and after it\n",
    "    text = remove_after_phrase(text, phrase=\"Read these next\")\n",
    "\n",
    "    # Remove text \"Download the HealthHub app\" and after it\n",
    "    text = remove_after_phrase(text, phrase=\"Download the HealthHub app\")\n",
    "\n",
    "    # Remove text \"close\\nclose\\nclose\"\n",
    "    text = remove_after_phrase(text, phrase=\"close\\nclose\\nclose\")\n",
    "\n",
    "    # Remove text \"Related Articles\" and after it\n",
    "    text = remove_after_phrase(text, phrase=\"Related Articles\")\n",
    "\n",
    "    # Remove text before \"A\\n A\\n A\\n\"\n",
    "    text = remove_before_phrase(text, phrase=\" A\\n A\\n A\\n\")\n",
    "\n",
    "    # Remove \"CONTRIBUTED BY ...\"\n",
    "    text = re.sub(r\"CONTRIBUTED BY\\n\\r\\n.*\\n*\", \"\", text)\n",
    "\n",
    "    # Fix merged words (e.g. Breast Cancer *ScreeningBeyond* the recommended…)\n",
    "    # But will split 'MediShield' -> 'Medi Shield'\n",
    "    text = re.sub(r\"(?<=\\w)([A-Z][a-z]+)\", r\" \\1\", text)\n",
    "\n",
    "    # Add space between symbols and words (e.g. )\n",
    "    text = re.sub(r\"([:.!?)])([A-Z])\", r\"\\1 \\2\", text)\n",
    "\n",
    "    # Remove \\n and \\r\n",
    "    text = re.sub(r\"[\\n\\r]\", \" \", text)\n",
    "\n",
    "    # Remove extra whitespaces\n",
    "    text = re.sub(r\" {2,}\", \" \", text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removal of stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text: str) -> str:\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    word_tokens = word_tokenize(text)\n",
    "    filtered_sentence = \" \".join(\n",
    "        [w for w in word_tokens if w.lower() not in stop_words]\n",
    "    )\n",
    "    return filtered_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"text\"] = df[\"text\"].apply(clean_data).apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop Rows that have Empty Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df[\"text\"] != \"\"]\n",
    "\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_chunk_id = df[\"chunk_id\"].tolist()\n",
    "loaded_source = df[\"doc_source\"].tolist()\n",
    "loaded_domain = df[\"doc_domain\"].tolist()\n",
    "loaded_title = df[\"doc_title\"].tolist()\n",
    "loaded_contributor = df[\"contributor\"].tolist()\n",
    "loaded_content = df[\"text\"].tolist()\n",
    "loaded_category = df[\"category\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6761 6761 6761 6761 6761 6761 6761\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    len(loaded_chunk_id),\n",
    "    len(loaded_source),\n",
    "    len(loaded_domain),\n",
    "    len(loaded_title),\n",
    "    len(loaded_contributor),\n",
    "    len(loaded_content),\n",
    "    len(loaded_category),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(CLEANED_CHUNK_ID_LIST_PATH, \"wb\") as file:\n",
    "    pickle.dump(loaded_chunk_id, file)\n",
    "\n",
    "with open(CLEANED_SOURCE_LIST_PATH, \"wb\") as file:\n",
    "    pickle.dump(loaded_source, file)\n",
    "\n",
    "with open(CLEANED_DOMAIN_LIST_PATH, \"wb\") as file:\n",
    "    pickle.dump(loaded_domain, file)\n",
    "\n",
    "with open(CLEANED_TITLE_LIST_PATH, \"wb\") as file:\n",
    "    pickle.dump(loaded_title, file)\n",
    "\n",
    "with open(CLEANED_CONTRIBUTOR_LIST_PATH, \"wb\") as file:\n",
    "    pickle.dump(loaded_contributor, file)\n",
    "\n",
    "with open(CLEANED_CONTENT_LIST_PATH, \"wb\") as file:\n",
    "    pickle.dump(loaded_content, file)\n",
    "\n",
    "with open(CLEANED_CATEGORY_LIST_PATH, \"wb\") as file:\n",
    "    pickle.dump(loaded_category, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mining",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  },
  "nbdime-conflicts": {
   "local_diff": [
    {
     "key": "language_info",
     "op": "add",
     "value": {
      "codemirror_mode": {
       "name": "ipython",
       "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
     }
    }
   ],
   "remote_diff": [
    {
     "key": "language_info",
     "op": "add",
     "value": {
      "codemirror_mode": {
       "name": "ipython",
       "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
     }
    }
   ]
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
