{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings (Traditional Statistical Vector-based Embeddings)\n",
    "\n",
    "Yes, traditional statistical vector-based embeddings are foundational techniques in natural language processing (NLP) that represent text data using various statistical measures. Here are some of these traditional methods:\n",
    "\n",
    "### 1. Bag of Words (BoW)\n",
    "- **Description**: Represents text by the occurrence (count) of each word in the document without considering the word order or context.\n",
    "- **Implementation**: Typically uses a Count Vectorizer.\n",
    "- **Characteristics**: Produces sparse vectors where each dimension corresponds to a specific term from the vocabulary and the value is the word count.\n",
    "- **Use Cases**: Simple and effective for basic text classification and clustering tasks.\n",
    "\n",
    "### 2. Term Frequency-Inverse Document Frequency (TF-IDF)\n",
    "- **Description**: Enhances the Bag of Words model by weighting terms based on their frequency in a document and their inverse frequency across all documents in the corpus.\n",
    "- **Implementation**: Uses TF-IDF Vectorizer.\n",
    "- **Characteristics**: Produces sparse vectors with weighted values, reducing the impact of common words and highlighting important terms.\n",
    "- **Use Cases**: Widely used in information retrieval and text mining.\n",
    "\n",
    "### 3. Latent Semantic Analysis (LSA) or Latent Semantic Indexing (LSI)\n",
    "- **Description**: Applies Singular Value Decomposition (SVD) to the term-document matrix (typically after applying TF-IDF) to reduce dimensions and capture latent semantic relationships between terms.\n",
    "- **Implementation**: Perform SVD on the term-document matrix.\n",
    "- **Characteristics**: Transforms high-dimensional sparse vectors into lower-dimensional dense vectors.\n",
    "- **Use Cases**: Useful for topic modeling and capturing underlying semantic structures.\n",
    "\n",
    "### 4. Latent Dirichlet Allocation (LDA)\n",
    "- **Description**: A generative probabilistic model that represents documents as mixtures of topics and topics as mixtures of words.\n",
    "- **Implementation**: Uses probabilistic algorithms to infer topic distributions.\n",
    "- **Characteristics**: Produces dense vectors representing the distribution of topics in each document.\n",
    "- **Use Cases**: Widely used for topic modeling and discovering abstract topics in large text corpora.\n",
    "\n",
    "### 5. Pointwise Mutual Information (PMI)\n",
    "- **Description**: Measures the association between a pair of words by comparing the probability of their co-occurrence to the probabilities of their individual occurrences.\n",
    "- **Implementation**: Uses co-occurrence matrices.\n",
    "- **Characteristics**: Produces dense vectors that capture the likelihood of words appearing together.\n",
    "- **Use Cases**: Useful for capturing word associations and semantic relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "from alive_progress import alive_bar\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statistical_methods = {\n",
    "    \"bow\": CountVectorizer,\n",
    "    \"tfidf\": TfidfVectorizer,\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "CONTRIBUTOR: str = \"Health Promotion Board\"\n",
    "CATEGORY: str = \"live-healthy\"\n",
    "MODEL_NAME: str = \"all-MiniLM-L6-v2\"\n",
    "POOLING_STRATEGY: str = \"max\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mining",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
