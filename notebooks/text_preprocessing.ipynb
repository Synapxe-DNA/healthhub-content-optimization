{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e25074f9-b04b-472c-a4b9-2027fd10b493",
   "metadata": {},
   "source": [
    "## Import Packages\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "5a9ab1d3-0877-4970-ad03-a6e222cfb44c",
   "metadata": {},
   "source": [
    "# Install required packages\n",
    "\n",
    "# !pip install openpyxl beautifulsoup4 lxml html5lib\n",
    "# !pip install htmlmin"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "0ca93081-aaee-4608-bfaf-22bbb4f6d432",
   "metadata": {},
   "source": [
    "# Import relevant packages\n",
    "\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import zipfile\n",
    "from io import StringIO\n",
    "from unicodedata import normalize\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# import htmlmin\n",
    "from markdownify import markdownify"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "d1f4fa46-58d1-4c0b-aebd-4798f2f7f83e",
   "metadata": {},
   "source": [
    "## Get .xlsx files from ZIP archive\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ea647b-141a-4ff8-929d-26523ed436f3",
   "metadata": {},
   "source": [
    "### Utility Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "2758f827-44d9-4d2b-ba89-3d442efed518",
   "metadata": {},
   "source": [
    "# Create zip archive\n",
    "\n",
    "\n",
    "def create_archive(zip_path):\n",
    "    archive = zipfile.ZipFile(zip_path, \"r\")\n",
    "    return archive"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "bd06f0ff-d7cf-4ba8-a33d-2ee745c0b4b5",
   "metadata": {},
   "source": [
    "# Get xlsx files from archive\n",
    "\n",
    "\n",
    "def get_xlsx_from_archive(archive):\n",
    "    files_and_dirs = archive.namelist()\n",
    "    xlsx_files = list(filter(lambda k: k.split(\".\")[-1] == \"xlsx\", files_and_dirs))\n",
    "    return xlsx_files"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ec10c7d2-7249-4999-82a6-cf97aff6e4c8",
   "metadata": {},
   "source": [
    "# Get file name\n",
    "\n",
    "\n",
    "def get_fname(fpath):\n",
    "    fname = fpath.split(\".\")[0].split(\"/\")[-1]\n",
    "    return fname"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "97985118-45fd-4dca-9035-fc763bedbeec",
   "metadata": {},
   "source": [
    "### Procedure\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "e2014d6d-3e2e-49e1-a300-ca00a255b3b8",
   "metadata": {},
   "source": [
    "# Open zip file and extract directories and files\n",
    "\n",
    "\n",
    "archive = create_archive(\"../data/GenAI - Full Content Export.zip\")\n",
    "files_and_dirs = archive.namelist()\n",
    "\n",
    "print(files_and_dirs)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "89c6b8a1-a2b4-4d70-9725-0813163834ce",
   "metadata": {},
   "source": [
    "# Get xlsx files from archive\n",
    "\n",
    "archive = create_archive(\"../data/GenAI - Full Content Export.zip\")\n",
    "xlsx_files = get_xlsx_from_archive(archive)\n",
    "\n",
    "print(xlsx_files)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "17381e9b-e1cf-49f4-ba7b-5c4aeb38ec6a",
   "metadata": {},
   "source": [
    "# Get file names from archive\n",
    "\n",
    "for xlsx in xlsx_files:\n",
    "    fname = get_fname(xlsx)\n",
    "    print(fname)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "af1ec938-5a75-495a-98aa-bc6575f0edd4",
   "metadata": {},
   "source": [
    "## Naive Exploratory Data Analysis (Single .xlsx file)\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "9bfd7691-3c99-401d-94cc-a22f603511c6",
   "metadata": {},
   "source": [
    "file = archive.open(xlsx_files[0])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "d5a9f245-67f8-4be9-bbfc-0c7c99dd25c6",
   "metadata": {},
   "source": [
    "### Display .xlsx as Pandas Dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "42eeefb6-ec6a-4d6d-92a1-021910cb2692",
   "metadata": {},
   "source": [
    "# Load excel file to dataframe\n",
    "\n",
    "df = pd.read_excel(file)\n",
    "\n",
    "# ruff: noqa: F821\n",
    "display(df)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c44e1676-a6de-422d-8fcf-070570a96693",
   "metadata": {},
   "source": [
    "# Display information on dataframe\n",
    "\n",
    "df.info()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "3bf86747-72d2-4cc3-9088-b91eaf244b91",
   "metadata": {},
   "source": [
    "### Filter Columns where all values are NaN\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "fb87ee59-27fc-4fdd-8cbf-9ef79cedec90",
   "metadata": {},
   "source": [
    "# Drop columns where all values are NaN (irrelevant columns)\n",
    "\n",
    "df_filtered = df.dropna(axis=\"columns\", how=\"all\")\n",
    "\n",
    "# ruff: noqa: F821\n",
    "display(df_filtered)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "73116c24-9994-4d4b-9010-a5cde828f3ca",
   "metadata": {},
   "source": [
    "# df_filtered.to_parquet('./export-published-cost-and-financing_14062024_data.parquet')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2d558f8c-4364-4732-bb46-a25bcdf58a42",
   "metadata": {},
   "source": [
    "# Get columns present\n",
    "\n",
    "print(df_filtered.columns)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "217abb09-51cb-41c7-b46d-ff5404d6c05e",
   "metadata": {},
   "source": [
    "### Find ContentBody Column in DataFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "5211e7db-6b0a-4372-9cb3-4f1a8010ae50",
   "metadata": {},
   "source": [
    "col = df_filtered.columns[df_filtered.columns.str.contains(\"ContentBody\")][0]\n",
    "print(col)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "cc6a2a87-ca0f-4b91-ae4c-4e8e474abe72",
   "metadata": {},
   "source": [
    "raw_html = df_filtered[col]\n",
    "\n",
    "# ruff: noqa: F821\n",
    "display(raw_html.head(15))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "ebd9fe8c-3753-425a-89e3-ce41afd90bdc",
   "metadata": {},
   "source": [
    "## Text Extraction\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "c94250e3-1753-4e50-a59c-8f9c201a74e6",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Extract HTML sample\n",
    "\n",
    "sample = raw_html[0]\n",
    "\n",
    "print(sample)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2ba64cab-f5d7-4d89-8308-b2c97c56d44a",
   "metadata": {},
   "source": [
    "print(sample is np.nan)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "08160afe-78b8-4831-94cc-72ee230f41cc",
   "metadata": {},
   "source": [
    "### Method 1: Scraping ContentBody using BeautifulSoup\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "012aa0a3-3dc1-47f5-a90c-216bc36518d5",
   "metadata": {},
   "source": [
    "soup = BeautifulSoup(sample, \"lxml\")\n",
    "\n",
    "# Some ContentBody values are wrapped with a div class HTML element\n",
    "if soup.div is not None:\n",
    "    soup.div.unwrap()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "38d34ba8-3c31-4336-b1c1-b39dd23bc204",
   "metadata": {},
   "source": [
    "clean_text = soup.get_text()\n",
    "print(clean_text)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "9c2015bf-0891-4ba3-a249-76c13f85410c",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "clean_text = (\n",
    "    normalize(\"NFKC\", clean_text).replace(\"\\n\\n\\n\", \"\\n\").replace(\"\\n\\n\\n\", \"\\n\\n\")\n",
    ")\n",
    "\n",
    "# clean_text = clean_text.split(\"\\n\")\n",
    "texts = clean_text.split(\"\\n\")\n",
    "\n",
    "for i in range(len(texts)):\n",
    "    texts[i] = texts[i].strip()\n",
    "\n",
    "print(texts)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3de91598-17c9-45c6-9ac5-1a563b294a3f",
   "metadata": {},
   "source": [
    "clean_text = \"\\n\".join(texts).strip()\n",
    "print(clean_text)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "11121310-7328-4255-aa6e-bc9e6a38f362",
   "metadata": {},
   "source": [
    "### Method 2: Scraping ContentBody using Regular Expressions\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "d1bc11ca-f1ef-4441-b07e-abaddb6665c5",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "CLEANR = re.compile(\"<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});\")\n",
    "# CLEANR = re.compile('<p.*?>|</p>|</div>|<div.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});')\n",
    "\n",
    "\n",
    "def clean_html(raw_html):\n",
    "    cleantext = re.sub(CLEANR, \" \", raw_html)\n",
    "    return cleantext\n",
    "\n",
    "\n",
    "clean_text = clean_html(sample)\n",
    "print(clean_text)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e6751c56-f217-4fea-952a-c0e83a659648",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "clean_text = (\n",
    "    normalize(\"NFKC\", clean_text).replace(\"\\n\\n\\n\", \"\\n\").replace(\"\\n\\n\\n\", \"\\n\\n\")\n",
    ")\n",
    "\n",
    "# clean_text = clean_text.split(\"\\n\")\n",
    "texts = clean_text.split(\"\\n\")\n",
    "\n",
    "for i in range(len(texts)):\n",
    "    texts[i] = texts[i].strip()\n",
    "\n",
    "print(texts)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "1fb78379-25be-44a2-9a7a-181bd2ac7093",
   "metadata": {},
   "source": [
    "clean_text = \"\\n\".join(texts).strip().replace(\"\\n\\n\\n\", \"\\n\").replace(\"\\n\\n\\n\", \"\\n\\n\")\n",
    "print(clean_text)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "114f2d6c-8364-43ea-8878-a0c2c783391c",
   "metadata": {},
   "source": [
    "### Method 3: Scraping ContentBody using Custom Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "f0a1cea5-8507-4e7c-940c-a28b49662fcc",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "soup = BeautifulSoup(sample, \"lxml\")\n",
    "\n",
    "print(soup)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "43f56d84-4129-4eb6-be83-927f52ed7997",
   "metadata": {},
   "source": [
    "#### Version 1 - Strip text + Introduce formatting\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "87ef885a-455f-4149-bc35-19a52cd5fd53",
   "metadata": {},
   "source": [
    "# Works very well but prone to duplicated text for nested tags. Look into resolving non-ascii representation of characters\n",
    "# TODO: Implement unicode normalisation when parsing text from HTML fragments\n",
    "\n",
    "\n",
    "def parse_text(soup):\n",
    "    def process_links(tag):\n",
    "        links = []\n",
    "\n",
    "        for a in tag.find_all(\"a\"):\n",
    "            title = a.get(\"title\") or a.text.strip()\n",
    "            url = a.get(\"href\")\n",
    "            links.append(f\"{title}: {url}\")\n",
    "\n",
    "        return links\n",
    "\n",
    "    def process_table(table):\n",
    "        if (\n",
    "            table.find_all(\"tr\") == []\n",
    "        ):  # Empty table in All You Need to Know About Childhood Immunisations\n",
    "            return \"\"\n",
    "\n",
    "        headers = [header.get_text(strip=True) for header in table.find_all(\"tr\")[0]]\n",
    "        headers = list(filter(lambda k: \" \" in k, headers))\n",
    "        rows = []\n",
    "\n",
    "        for row in table.find_all(\"tr\")[1:]:\n",
    "            cols = row.find_all(\"td\")\n",
    "            cols = [ele.get_text(strip=True).replace(\"\\xa0\", \" \") for ele in cols]\n",
    "            rows.append(cols)\n",
    "\n",
    "        table_text = []\n",
    "\n",
    "        if headers:\n",
    "            table_text.append(\" | \".join(headers))\n",
    "\n",
    "        for row in rows:\n",
    "            table_text.append(\" | \".join(row))\n",
    "\n",
    "        return \"\\n\".join(table_text)\n",
    "\n",
    "    if soup.div is not None:\n",
    "        soup.div.unwrap()\n",
    "\n",
    "    # TODO: Implement unicode normalisation when parsing text from HTML fragments\n",
    "    organized_text = []\n",
    "\n",
    "    for elem in soup.find_all(\n",
    "        [\"p\", \"h1\", \"h2\", \"h3\", \"h4\", \"h5\", \"h6\", \"div\", \"ul\", \"table\", \"ol\"]\n",
    "    ):\n",
    "        if elem.name == \"p\":\n",
    "            organized_text.append(elem.text.strip())\n",
    "\n",
    "        elif elem.name in [\"h1\", \"h2\", \"h3\", \"h4\", \"h5\", \"h6\"]:\n",
    "            organized_text.append(f\"\\n\\n{elem.text.strip()}\\n\")\n",
    "\n",
    "        elif elem.name == \"div\":\n",
    "            organized_text.append(elem.text.strip())\n",
    "\n",
    "        elif elem.name == \"ul\":\n",
    "            for li in elem.find_all(\"li\"):\n",
    "                organized_text.append(f\"  - {li.text.strip()}\")\n",
    "\n",
    "        elif elem.name == \"ol\":\n",
    "            for i, li in enumerate(elem.find_all(\"li\"), 1):\n",
    "                organized_text.append(f\"  {i}. {li.text.strip()}\")\n",
    "\n",
    "        elif elem.name == \"table\":\n",
    "            organized_text.append(process_table(elem))\n",
    "\n",
    "        links = process_links(elem)\n",
    "        if links:\n",
    "            organized_text.append(\"\\n\")\n",
    "            organized_text.extend(links)\n",
    "            organized_text.append(\"\\n\")\n",
    "\n",
    "    # print(organized_text)\n",
    "    return (\n",
    "        \"\\n\".join(organized_text)\n",
    "        .replace(\"\\n\\n\", \"\\n\")\n",
    "        .replace(\"\\n         \", \"\")\n",
    "        .replace(\"\\n      \", \"\")\n",
    "        .replace(\"\\u200b\", \"\")\n",
    "        .strip()\n",
    "    )\n",
    "\n",
    "\n",
    "print(parse_text(soup))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "badb3b09-f07c-4605-8e7a-661f4542c445",
   "metadata": {},
   "source": [
    "#### Version 2 - Introduce Unicode Normalisation + Encode to ASCII + Decode back to UTF-8\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "f29da098-ae90-4e01-b204-e6117dba74e7",
   "metadata": {},
   "source": [
    "# Works very well but prone to duplicated text for nested tags. Look into resolving non-ascii representation of characters\n",
    "# TODO: Implement unicode normalisation when parsing text from HTML fragments\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    clean_text = (\n",
    "        normalize(\"NFKC\", text).encode(\"ascii\", \"ignore\").decode(\"utf8\").strip()\n",
    "    )\n",
    "\n",
    "    return clean_text\n",
    "\n",
    "\n",
    "def process_links(tag):\n",
    "    links = []\n",
    "\n",
    "    for a in tag.find_all(\"a\"):\n",
    "        title = a.get(\"title\") or a.text.strip()\n",
    "        url = a.get(\"href\")\n",
    "        links.append(f\"{title}: {url}\")\n",
    "\n",
    "    return links\n",
    "\n",
    "\n",
    "def process_table(table):\n",
    "    if (\n",
    "        table.find_all(\"tr\") == []\n",
    "    ):  # Empty table in All You Need to Know About Childhood Immunisations\n",
    "        return \"\"\n",
    "\n",
    "    headers = [clean_text(header.get_text()) for header in table.find_all(\"tr\")[0]]\n",
    "    headers = list(filter(lambda k: \" \" in k, headers))\n",
    "    rows = []\n",
    "\n",
    "    for row in table.find_all(\"tr\")[1:]:\n",
    "        cols = row.find_all(\"td\")\n",
    "        cols = [clean_text(ele.get_text()) for ele in cols]\n",
    "        rows.append(cols)\n",
    "\n",
    "    table_text = []\n",
    "\n",
    "    if headers:\n",
    "        table_text.append(\" | \".join(headers))\n",
    "\n",
    "    for row in rows:\n",
    "        table_text.append(\" | \".join(row))\n",
    "\n",
    "    return \"\\n\".join(table_text)\n",
    "\n",
    "\n",
    "def parse_text(soup):\n",
    "    if soup.div is not None:\n",
    "        soup.div.unwrap()\n",
    "\n",
    "    # TODO: Implement unicode normalisation when parsing text from HTML fragments\n",
    "    organized_text = []\n",
    "\n",
    "    for elem in soup.find_all(\n",
    "        [\"p\", \"h1\", \"h2\", \"h3\", \"h4\", \"h5\", \"h6\", \"div\", \"ul\", \"table\", \"ol\"]\n",
    "    ):\n",
    "        text = clean_text(elem.text)\n",
    "        if elem.name in [\"p\", \"div\"]:\n",
    "            organized_text.append(text)\n",
    "\n",
    "        elif elem.name in [\"h1\", \"h2\", \"h3\", \"h4\", \"h5\", \"h6\"]:\n",
    "            organized_text.append(f\"\\n\\n{text}\\n\")\n",
    "\n",
    "        elif elem.name == \"ul\":\n",
    "            for li in elem.find_all(\"li\"):\n",
    "                li_text = clean_text(li.text)\n",
    "                organized_text.append(f\"  - {li_text}\")\n",
    "\n",
    "        elif elem.name == \"ol\":\n",
    "            for i, li in enumerate(elem.find_all(\"li\"), 1):\n",
    "                li_text = clean_text(li.text)\n",
    "                organized_text.append(f\"  {i}. {li_text}\")\n",
    "\n",
    "        elif elem.name == \"table\":\n",
    "            organized_text.append(process_table(elem))\n",
    "\n",
    "        links = process_links(elem)\n",
    "        if links:\n",
    "            organized_text.append(\"\\n\\n\")\n",
    "            organized_text.extend(links)\n",
    "            organized_text.append(\"\\n\\n\")\n",
    "\n",
    "    # print(organized_text)\n",
    "    # return \"\\n\".join(organized_text).strip()\n",
    "    return (\n",
    "        \"\\n\".join(organized_text)\n",
    "        .replace(\"\\n\\n\\n\", \"\\n\")\n",
    "        .replace(\"\\n            \", \"\")\n",
    "        .replace(\"\\n      \", \"\")\n",
    "        .strip()\n",
    "    )\n",
    "\n",
    "\n",
    "print(parse_text(soup))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ae2927ab-5994-4268-833b-6ced64904544",
   "metadata": {},
   "source": [
    "# Extract tables - Experimental\n",
    "\n",
    "\n",
    "# Notes: Need to check if table exists\n",
    "def extract_table(sample):\n",
    "    html_file = StringIO(sample)\n",
    "\n",
    "    tables = pd.read_html(html_file, header=0)\n",
    "    stores = []\n",
    "\n",
    "    for i in range(len(tables)):\n",
    "        json_str = tables[0].to_json(index=False, orient=\"records\")\n",
    "        store = json.loads(json_str)\n",
    "\n",
    "        for i in range(len(store)):\n",
    "            ele = store[i]\n",
    "            for key, value in ele.items():\n",
    "                ele[key] = normalize(\"NFKC\", str(value).replace(\"\\u200b\", \"\"))\n",
    "            store[i] = ele\n",
    "\n",
    "        stores.append(store)\n",
    "\n",
    "    if not stores:\n",
    "        return None\n",
    "\n",
    "    return stores\n",
    "\n",
    "\n",
    "# print(extract_table(sample))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2bec84ff-1d89-4a45-8a0a-1b1dd14d9587",
   "metadata": {},
   "source": [
    "# minified = htmlmin.minify(sample, remove_empty_space=True)\n",
    "# print(minified)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b2bdaa99-eb4c-49d1-8bac-dc9d30dd5225",
   "metadata": {},
   "source": [
    "# Parse HTML via BeautifulSoup\n",
    "\n",
    "soup = BeautifulSoup(sample, \"lxml\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "7dd8d388-cc7c-4c67-aeb8-7054632d1727",
   "metadata": {},
   "source": [
    "def extract_tags(soup):\n",
    "    tags = set()\n",
    "\n",
    "    for tag in soup.find_all(True):\n",
    "        tags.add(tag.name)\n",
    "\n",
    "    return list(tags)\n",
    "\n",
    "\n",
    "print(extract_tags(soup))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "1eb2d79a-9878-470c-9996-cdfe96f5467b",
   "metadata": {},
   "source": [
    "# Display all headers from article}\n",
    "\n",
    "\n",
    "def extract_headers(soup):\n",
    "    titles = soup.find_all([\"h1\", \"h2\", \"h3\", \"h4\", \"h5\", \"h6\"])\n",
    "\n",
    "    headers = []\n",
    "    for title in titles:\n",
    "        tag = title.name\n",
    "        text = title.get_text()\n",
    "        records = text, tag\n",
    "        headers.append(records)\n",
    "\n",
    "    # print('List all the header tags :', *titles, sep='\\n\\n', end=\"\\n\\n\")\n",
    "    return headers\n",
    "\n",
    "\n",
    "print(extract_headers(soup))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "0ad1e76c-c62a-4c6d-93fd-914413084b03",
   "metadata": {},
   "source": [
    "def extract_urls_list(soup):\n",
    "    url_records = []\n",
    "\n",
    "    for link in soup.find_all(\"a\"):\n",
    "        url = link.get(\"href\")\n",
    "        title = link.get(\"title\")\n",
    "        text = link.get_text()\n",
    "\n",
    "        if title is not None:\n",
    "            records = title, url\n",
    "        else:\n",
    "            records = text, url\n",
    "\n",
    "        url_records.append(records)\n",
    "\n",
    "    if not url_records:\n",
    "        return None\n",
    "\n",
    "    return url_records\n",
    "\n",
    "\n",
    "print(extract_urls_list(soup))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a2c5e009-87df-442e-a374-88cc324a6b07",
   "metadata": {},
   "source": [
    "def extract_urls_dict(soup):\n",
    "    urls_dict = dict()\n",
    "    for link in soup.find_all(\"a\"):\n",
    "        url = link.get(\"href\")\n",
    "        title = link.get(\"title\")\n",
    "        text = link.get_text()\n",
    "\n",
    "        if url not in urls_dict:\n",
    "            if title is not None:\n",
    "                urls_dict[url] = title\n",
    "            else:\n",
    "                urls_dict[url] = text\n",
    "\n",
    "    res = dict((v, k) for k, v in urls_dict.items())\n",
    "\n",
    "    if not res:\n",
    "        return None\n",
    "\n",
    "    return res\n",
    "\n",
    "\n",
    "print(extract_urls_dict(soup))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b8070ff8-3be4-4216-8aea-184587f99c2a",
   "metadata": {},
   "source": [
    "def extract_info(sample):\n",
    "    if sample is np.nan:\n",
    "        return\n",
    "\n",
    "    soup = BeautifulSoup(sample, \"lxml\")\n",
    "\n",
    "    tags = extract_tags(soup)\n",
    "    headers = extract_headers(soup)\n",
    "    urls = extract_urls_list(soup)\n",
    "\n",
    "    tables = None\n",
    "    if soup.find_all(\"table\") != []:\n",
    "        try:\n",
    "            tables = extract_table(sample)\n",
    "            # tables_str = json.dumps(tables)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "\n",
    "    text = parse_text(soup)\n",
    "    return tags, headers, urls, tables, text\n",
    "\n",
    "\n",
    "print(extract_info(sample))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "6d80b20b-1ac6-48be-92fe-542293385b18",
   "metadata": {},
   "source": [
    "def write_to_txt(dir_path, fname, text):\n",
    "    fpath = f\"{dir_path}/{fname}.txt\"\n",
    "\n",
    "    try:\n",
    "        os.makedirs(dir_path, exist_ok=True)\n",
    "        with open(fpath, \"w\") as f:\n",
    "            f.write(text)\n",
    "    except OSError as error:\n",
    "        print(error)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a1681b7d-bd7c-4308-aa89-2cdac40dcf4d",
   "metadata": {},
   "source": [
    "def process_xlsx(archive, xlsx_file, dir_path):\n",
    "    dname = xlsx_file.split(\".\")[0].split(\"/\")[-1]\n",
    "    output_dir = f\"{dir_path}/{dname}\"\n",
    "    print(\"\\n\\n\", output_dir)\n",
    "\n",
    "    file = archive.open(xlsx_file)\n",
    "    df = pd.read_excel(file)\n",
    "    df_processed = df.dropna(axis=\"columns\", how=\"all\")\n",
    "\n",
    "    # \"Content_Body\" instead of \"ContentBody\" in export-published-live-healthy-articles_14062024_data.xlsx\n",
    "    # \"Content_x0020_Body\" instead of \"ContentBody\" in export-published-programs_14062024_data.xlsx\n",
    "    col = df_processed.columns[\n",
    "        df_processed.columns.str.contains(\"ContentBody\")\n",
    "        | df_processed.columns.str.contains(\"Content_Body\")\n",
    "        | df_processed.columns.str.contains(\"Content_x0020_Body\")\n",
    "    ][0]\n",
    "    article_names = df_processed[\"Content.Name\"]\n",
    "    raw_htmls = df_processed[col]\n",
    "\n",
    "    for i in range(n := len(raw_htmls)):\n",
    "        sample = raw_htmls[i]\n",
    "        # Some articles uses slashes for medications\n",
    "        fname = article_names[i].replace(\"/\", \"-\")\n",
    "        print(fname)\n",
    "\n",
    "        if sample is not np.nan:\n",
    "            tags, headers, urls, tables, text = extract_info(sample)\n",
    "            write_to_txt(output_dir, fname, text)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "34ab5318-cdf9-4a17-b789-aad8184bdbbf",
   "metadata": {},
   "source": [
    "# Function to extract xlsx files from archive\n",
    "\n",
    "\n",
    "def create_archive(zip_path):\n",
    "    archive = zipfile.ZipFile(zip_path, \"r\")\n",
    "    return archive\n",
    "\n",
    "\n",
    "def get_xlsx_from_archive(archive):\n",
    "    files_and_dirs = archive.namelist()\n",
    "    xlsx_files = list(filter(lambda k: k.split(\".\")[-1] == \"xlsx\", files_and_dirs))\n",
    "    return xlsx_files\n",
    "\n",
    "\n",
    "print(get_xlsx_from_archive(create_archive(\"../data/GenAI - Full Content Export.zip\")))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b82821c9-0a14-4353-865e-e306e9b82559",
   "metadata": {},
   "source": [
    "def main():\n",
    "    archive_path = \"../data/GenAI - Full Content Export.zip\"\n",
    "    output_dir_path = \"../data/processed\"\n",
    "    archive = create_archive(archive_path)\n",
    "    xlsx_files = get_xlsx_from_archive(archive)\n",
    "\n",
    "    for i in range(len(xlsx_files)):\n",
    "        process_xlsx(archive, xlsx_files[i], output_dir_path)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "9fb15fc8-53b9-4da6-82b4-0c8e936e15b9",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "main()\n",
    "\n",
    "# # Errors:\n",
    "# invalid literal for int() with base 10: 'h2'\n",
    "# [Errno 63] File name too long: '../data/processed/export-published-live-healthy-articles_14062024_data/                                                                                                                                                                                                         Books for your growing child (Toddler and Preschooler).txt'\n",
    "# No tables found matching pattern '.+'\n",
    "# /var/folders/3n/y5_h0fxs0bv2mhb7bf_fpmb80000gn/T/ipykernel_53004/2874476162.py:5: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
    "#   soup = BeautifulSoup(sample, 'lxml')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "dcf156b2-3900-47b3-82d1-b0565e40d40f",
   "metadata": {},
   "source": [
    "### Method 4: Convert HTML to Markdown\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "a677a79a-f305-4959-8e1e-cd8116ed46d7",
   "metadata": {},
   "source": [
    "# !pip install markdownify"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c847aa38-085e-4fa5-ade1-493c186348eb",
   "metadata": {},
   "source": [
    "# Convert HTML to Markdown\n",
    "markdown_text = markdownify(sample)\n",
    "\n",
    "# Display converted text\n",
    "print(markdown_text.replace(\"\\n\\n\\n\", \"\\n\\n\").strip())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "b4aaf82a-296f-4b30-aa08-d6a0ca013f55",
   "metadata": {},
   "source": [
    "# END\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "829476b9-13f3-4996-b0e7-10a6b3cd50af",
   "metadata": {},
   "source": [
    "---\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
