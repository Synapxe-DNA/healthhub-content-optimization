{
  "code": "def generate_subclusters(\n    weighted_embeddings: pd.DataFrame,\n    first_level_pred_cluster: pd.DataFrame,\n    umap_parameters: Dict,\n    size_threshold: float,\n) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n    \"\"\"\n    Generate subclusters from the first level of clustering using BERTopic.\n\n    This function processes the given clusters and embeddings to perform a second level of clustering\n    on clusters that have more than 10 elements. It merges the resulting subclusters with the original\n    predictions and assigns unique topic numbers to them. Additionally, it generates cluster keywords\n    for clusters that did not undergo the second-level clustering but have more than one element.\n\n    Parameters:\n    ----------\n    weighted_embeddings (pd.DataFrame): A DataFrame containing the weighted embeddings with 'id' and 'extracted_content_body_embeddings' columns.\n    first_level_pred_cluster (pd.DataFrame): A DataFrame containing the predicted clusters with 'id' and 'cluster' columns.\n    umap_parameters (Dict): A dictionary of UMAP parameters (n_neighbors, n_components) to be used for the clustering process.\n    size_threshold (float): The minimum number of articles a cluster must have to proceed with subclustering.\n\n    Returns:\n    -------\n    final_predicted_cluster (pd.DataFrame): A DataFrame with the updated clusters, including the new subclusters and cluster keywords.\n    final_cluster_size (pd.DataFrame): DataFrame containing the size of each cluster in bins of size 5.\n    final_metrics (pd.DataFrame): DataFrame containing clustering metrics. Including no. of clusters, min & max cluster size and no. of unclustered articles.\n    \"\"\"\n    cluster_size_count = first_level_pred_cluster.cluster.value_counts()\n    to_keep = cluster_size_count[cluster_size_count > size_threshold].index\n    cluster_morethan_threshold = first_level_pred_cluster[\n        first_level_pred_cluster.cluster.isin(to_keep)\n    ]\n    cluster_morethan_threshold_embeddings = pd.merge(\n        cluster_morethan_threshold,\n        weighted_embeddings[[\"id\", \"extracted_content_body_embeddings\"]],\n        how=\"left\",\n        on=\"id\",\n    )\n\n    print(\n        \"No. of clusters to go through 2nd level clustering: \",\n        cluster_morethan_threshold.cluster.nunique(),\n    )\n    final_result_df = process_all_clusters(\n        cluster_morethan_threshold_embeddings, umap_parameters\n    )\n    final_result_df_with_numbers = assign_unique_numbers_to_topics(\n        final_result_df, first_level_pred_cluster\n    )\n    new_cluster_to_merge = final_result_df_with_numbers[\n        [\"id\", \"top_5_kws\", \"Assigned Topic Number\"]\n    ]\n    new_cluster_to_merge.columns = [\"id\", \"cluster_kws\", \"new_cluster\"]\n    final_predicted_cluster = pd.merge(\n        first_level_pred_cluster, new_cluster_to_merge, how=\"left\", on=\"id\"\n    )\n    final_predicted_cluster[\"new_cluster\"] = (\n        final_predicted_cluster[\"new_cluster\"]\n        .fillna(final_predicted_cluster[\"cluster\"])\n        .apply(int)\n    )\n\n    # Generate cluster kws for those that did not undergo bertopic (cluster_kws is na) & cluster size > 1 (cluster_mt_1)\n    unique_new_clusters = final_predicted_cluster[\"new_cluster\"].value_counts()\n    cluster_mt_1 = unique_new_clusters[unique_new_clusters > 1].index\n    df_cluster_kws_na = final_predicted_cluster[\n        final_predicted_cluster[\"new_cluster\"].isin(cluster_mt_1)\n        & final_predicted_cluster[\"cluster_kws\"].isna()\n    ]\n    cluster_kws_dict = generate_cluster_keywords(df_cluster_kws_na)\n    final_predicted_cluster[\"cluster_kws\"] = final_predicted_cluster[\n        \"cluster_kws\"\n    ].fillna(final_predicted_cluster[\"new_cluster\"].map(cluster_kws_dict))\n    final_cluster_size = get_cluster_size(\n        final_predicted_cluster, column_name=\"new_cluster\"\n    )\n\n    value_count_cluster_size = final_predicted_cluster[\"new_cluster\"].value_counts()\n    min_count = value_count_cluster_size[value_count_cluster_size != 1].min()\n    max_count = value_count_cluster_size.max()\n    num_clusters = (value_count_cluster_size[value_count_cluster_size != 1] != 1).sum()\n    unclustered_count = (value_count_cluster_size == 1).sum()\n\n    final_metrics = pd.DataFrame(\n        {\n            \"Number of clusters\": [num_clusters],\n            \"Min cluster size\": [min_count],\n            \"Max cluster size\": [max_count],\n            \"Number of articles not clustered\": [unclustered_count],\n        }\n    )\n\n    return final_predicted_cluster, final_cluster_size, final_metrics\n",
  "filepath": "content-optimization/src/content_optimization/pipelines/clustering/nodes.py",
  "parameters": {
    "umap_parameters": {
      "n_neighbors": 15,
      "n_components": 8
    },
    "size_threshold": 10
  },
  "run_command": "kedro run --to-nodes='generate_subclusters_node'",
  "inputs": [
    "weighted_embeddings",
    "first_level_pred_cluster",
    "params:umap_parameters",
    "params:size_threshold"
  ],
  "outputs": [
    "final_predicted_cluster",
    "final_cluster_size",
    "final_metrics"
  ]
}