# Content Optimization for HealthHub

This repository contains the content optimization for HealthHub.

## Installation

Create a virtual environment using `conda` (recommended) and install dependencies:

```bash
# Create a virtual environment
conda create -n <VENV_NAME> python=3.12 -y
conda activate <VENV_NAME>

# Install dependencies
pip install -r requirements.txt
```

## File Structure

The exploratory/experimental code for content optimization is stored in the [`notebooks/`](notebooks) folder.

- [`artifacts/`](artifacts): contains the output of the exploratory/experimental code

    * [`notebooks/`](artifacts/notebooks): contains experiments generated by [`papermill`](https://papermill.readthedocs.io/en/latest/)

    * [`outputs/`](artifacts/outputs): contains the experiment outputs (i.e. confusion matrices) generated by [`papermill`](https://papermill.readthedocs.io/en/latest/)

        * [`statistical_vector_based_embeddings_similarity_scores.xlsx`](artifacts/outputs/statistical_vector_based_embeddings_similarity_scores.xlsx): contains the similarity scores of experiments generated from Statistical Vector-based Embedding techniques.

- [`data/`](data): contains the data used in the exploratory/experimental code

    * [`healthhub_small/`](data/healthhub_small): contains a small subset of Health Hub raw data

    * [`healthhub_small_clean/`](data/healthhub_small_clean): contains the small subset of Health Hub cleaned data; also stores the embeddings generated from Sentence Transformers in a `parquet` format.

- [`notebooks/`](notebooks): contains the exploratory/experimental code where bulk of the logic is implemented

    * [`logger.py`](notebooks/logger.py): contains the code for logging

    * [`preprocess.ipynb`](notebooks/preprocess.ipynb): contains the code for preprocessing the raw data; cleaned output will be stored in [`healthhub_small_clean/`](data/healthhub_small_clean); only needed to run once

    * [`embeddings.ipynb`](notebooks/embeddings.ipynb): contains the code for generating embeddings; embeddings will be stored in [`healthhub_small_clean/`](data/healthhub_small_clean)

    * [`similarity.ipynb`](notebooks/similarity.ipynb): contains the code for calculating similarity between embeddings

    * [`runner.ipynb`](notebooks/runner.ipynb): contains the code for running the notebooks — [`embeddings.ipynb`](notebooks/embeddings.ipynb) and [`similarity.ipynb`](notebooks/similarity.ipynb); parameterize by [`papermill`](https://papermill.readthedocs.io/en/latest/); this notebook helps you run your experiments for different models and pooling strategies and evaluate the results in the `artifacts/` folder

    * [`emb_sim_statistical.ipynb`](notebooks/emb_sim_statistical.ipynb): contains the code for generating embeddings from Statistical Vector-based Embeddings (SVE) techniques and calculating the similarity between embeddings

    * [`runner_statistical.ipynb`](notebooks/runner_statistical.ipynb): contains the code for running the notebook — [`emb_sim_statistical.ipynb`](notebooks/emb_sim_statistical.ipynb); parameterize by [`papermill`](https://papermill.readthedocs.io/en/latest/); this notebook helps you run your experiments for different SVE techniques and similarity metrics and evaluate the results in the `artifacts/` folder

## Usage

To run the notebooks, you can use the [`runner.ipynb`](notebooks/runner.ipynb) or [`runner_statistical.ipynb`](notebooks/runner_statistical.ipynb):

```python
# runner.ipynb

import papermill as pm
from logger import logger

pm.inspect_notebook("<INPUT_NOTEBOOK>")  # inspects and outputs the notebook's parameters

pm.execute_notebook(
    input_path="<INPUT_NOTEBOOK>",  # input notebook path
    output_path="<OUTPUT_NOTEBOOK>",  # input notebook path
    parameters={...},  # parameters to be passed to the notebook in a dictionary
```

## Pushing to GitHub

> **⚠️ Warning:** Refrain from pushing into `main` branch directly — it is bad practice. Always create a new branch and make your changes on your new branch.

Every time you complete a feature or change on a branch and want to push it to GitHub to make a pull request, you need to ensure you lint your code.

You can simply run the command `pre-commit run --all-files` to lint your code. For more information, refer to the [pre-commit docs](https://pre-commit.com/). To see what linters are used, refer to the [`.pre-commit-config.yaml`](.pre-commit-config.yaml) YAML file.

You should ensure that all cases are satisfied before you push to GitHub (you should see that all has passed). If not, please debug accordingly or your pull request may be rejected and closed.

The [`lint.yml`](.github/workflows/lint.yml) is a GitHub workflow that kicks off several GitHub Actions when a pull request is made. These GitHub Actions check that your code have been properly linted before it is passed for review. Once all actions have passed and the PR approved, your changes will be merged to the `main` branch.

> **Note:** The `pre-commit` will run regardless if you forget to explicitly call it. Nonetheless, it is recommended to call it explicitly so you can make any necessary changes in advanced.
